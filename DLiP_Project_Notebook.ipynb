{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726,"isSourceIdPinned":false},{"sourceId":11473304,"sourceType":"datasetVersion","datasetId":7190440},{"sourceId":11658923,"sourceType":"datasetVersion","datasetId":7316517,"isSourceIdPinned":false},{"sourceId":9251202,"sourceType":"datasetVersion","datasetId":5596873}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/annaattuch/dlip-notebook-group-4?scriptVersionId=285498832\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background-color:#5F9EA0; \n            color:white; \n            padding:15px; \n            border-radius:10px; \n            text-align:center; \n            font-size:30px; \n            font-weight:bold;\">\n    Detecting Deep Fakes - A Deep Learning Computer Vision Task<br>\n    <span style=\"font-size:20px; font-weight:normal;\">\n        Hairy Feet: Anna, Jacob, Johannes\n    </span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n    1. Introduction: Project Overview & Data \n</h1","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nIn this project, we build a deep learning computer vision model to distinguish between real human faces, and AI-generated (deepfake) face images. \n\nWe use the **DeepDetect-2025** dataset from Kaggle, which contains over 100k labeled images of faces, split into two classes: real and fake. The goal is to train a binary classifier that can automatically detect whether an image is genuine or AI generated.","metadata":{}},{"cell_type":"markdown","source":"# 2. Methods","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Importing and Preparing Data","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download CIFAKE dataset (for later use)\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download Deepfake 20K\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download Human-faces dataset\npath = kagglehub.dataset_download(\"/kaggle/input/human-faces-dataset\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:49:21.822971Z","iopub.execute_input":"2025-12-11T09:49:21.823259Z","iopub.status.idle":"2025-12-11T09:49:22.808956Z","shell.execute_reply.started":"2025-12-11T09:49:21.823237Z","shell.execute_reply":"2025-12-11T09:49:22.808399Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/deepdetect-2025\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n\nThe DeepDetect-2025 dataset is organized into separate \"train\" and \"test\" folders, each containing two subfolders:\n\n- \"real\" = real human face images  \n- \"fake\" = AI-generated (deepfake) face images  \n\nWe will:\n- Set a fixed image size and batch size\n- Point TensorFlow to the \"train\" and \"test\" directories\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndeepdetect_train_dir = \"/kaggle/input/deepdetect-2025/ddata/train\"\ndeepdetect_test_dir  = \"/kaggle/input/deepdetect-2025/ddata/test\"\n\ncifake_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\ncifake_test_dir  = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test\"\n\ndf20k_dir = \"/kaggle/input/deepfake-vs-real-20k/Deep-vs-Real\"\n\nhf_dir = \"/kaggle/input/human-faces-dataset/Human Faces Dataset\"\n\nds200k_dir = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/my_real_vs_ai_dataset/my_real_vs_ai_dataset\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:41:31.381625Z","iopub.execute_input":"2025-12-11T18:41:31.382068Z","iopub.status.idle":"2025-12-11T18:41:51.009322Z","shell.execute_reply.started":"2025-12-11T18:41:31.382045Z","shell.execute_reply":"2025-12-11T18:41:51.008524Z"}},"outputs":[{"name":"stderr","text":"2025-12-11 18:41:33.047760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765478493.283075      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765478493.349545      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"\n<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nNow we create the training, validation and test sets. \nWe use tf.keras.utils.image_dataset_from_directory to:\n\n- Load images from the \"train\"  directory\n- Automatically split the training data into:\n  - **80% training**\n  - **20% validation**\n\nThe images in the \"test\" directory are used as a separate held-out test set that we will only use for final evaluation.","metadata":{}},{"cell_type":"code","source":"# Helper function for getting dataset\ndef create_ds(directory, subset, labels, IMG_SIZE=(224,224), BATCH_SIZE=None, shuffle=True):\n    ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        validation_split=0.2 if subset else None, # Only split if subset is asked for\n        subset=subset,\n        seed=2025,\n        class_names=labels,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        label_mode='int',\n        shuffle=shuffle # <--- Pass this through\n    )\n    return ds\n\n# --- CIFAKE ---\nds_train_cifake = create_ds(cifake_train_dir, 'training', ['FAKE', 'REAL'])\nds_val_cifake   = create_ds(cifake_train_dir, 'validation', ['FAKE', 'REAL'])\n\nds_train_cifake = ds_train_cifake.apply(tf.data.experimental.ignore_errors()) # Because it has corrupted files that crash the system otherwise\n\n#-- 200K Dataset--\nds_train_200k = create_ds(ds200k_dir, 'training', ['ai_images', 'real'])\nds_val_200k   = create_ds(ds200k_dir, 'validation', ['ai_images', 'real'])\n\n# CIFAKE hold-out set\nds_test_cifake = create_ds(cifake_test_dir, None, ['FAKE', 'REAL'], shuffle = False)\n\n# Out-of-Distribution (face) sets\n\n# --- Deepfake 20k ---\nds_ood_df20k = create_ds(df20k_dir, None, ['Deepfake', 'Real'], shuffle = False)\n\n# --- HF Dataset ---\nds_ood_hf = create_ds(hf_dir, None , ['AI-Generated Images', 'Real Images'], shuffle = False)\n\n# 2. Concatenate them (Append HF to the end of DF20k)\nds_ood_combined = ds_ood_df20k.concatenate(ds_ood_hf)\n\nprint(\"Datasets created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:55:29.454711Z","iopub.execute_input":"2025-12-11T18:55:29.454975Z","iopub.status.idle":"2025-12-11T18:56:37.179489Z","shell.execute_reply.started":"2025-12-11T18:55:29.454957Z","shell.execute_reply":"2025-12-11T18:56:37.178155Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4259493749.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- CIFAKE ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mds_train_cifake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifake_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mds_val_cifake\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcreate_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifake_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/4259493749.py\u001b[0m in \u001b[0;36mcreate_ds\u001b[0;34m(directory, subset, labels, IMG_SIZE, BATCH_SIZE, shuffle)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Helper function for getting dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Only split if subset is asked for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Define a helper to enforce shapes\ndef force_shape(image, label):\n    # Explicitly set the shape. This converts RaggedTensors to Dense Tensors.\n    image = tf.ensure_shape(image, (224, 224, 3))\n    label = tf.ensure_shape(label, ())\n    return image, label\n\n# Full dataset function\ndef ds_full(datasets, weights, batch_size = 64):\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    ds_full = tf.data.Dataset.sample_from_datasets(\n        datasets,\n        weights=weights, # Optional: Balance the datasets if one is huge\n        stop_on_empty_dataset=False\n    )\n    ds_full = ds_full.map(force_shape, num_parallel_calls=tf.data.AUTOTUNE)\n\n    ds_full = ds_full.shuffle(buffer_size=1000)\n    ds_full = ds_full.batch(batch_size)\n    ds_full = ds_full.prefetch(tf.data.AUTOTUNE)\n\n    return ds_full\n\n# Test set function\ndef prepare_test_set(ds, batch_size=64):\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    # 1. Force Shape (Same as before)\n    ds = ds.map(force_shape, num_parallel_calls=AUTOTUNE)\n    \n    # 2. Batch (Critical)\n    ds = ds.batch(batch_size)\n    \n    # 3. Prefetch (Speed)\n    ds = ds.prefetch(AUTOTUNE)\n    \n    # NOTE: No shuffle(), No sample_from_datasets()\n    return ds\n\n# Full training set\nds_train_full = ds_full([ds_train_cifake, ds_train_200k], [1.0, 1.0])\nprint(\"Train Dataset Created!\")\n\n# Full validation set\nds_val_full = ds_full([ds_val_cifake, ds_val_200k], [1.0, 1.0])\nprint(\"Validation Dataset Created!\")\n\n# Test set\nds_test_cifake = prepare_test_set(ds_test_cifake)\n\nds_test_ood = prepare_test_set(ds_ood_combined)\nPrint(\"Test Datasets Created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:51:55.127943Z","iopub.execute_input":"2025-12-11T18:51:55.128624Z","iopub.status.idle":"2025-12-11T18:51:55.319233Z","shell.execute_reply.started":"2025-12-11T18:51:55.128594Z","shell.execute_reply":"2025-12-11T18:51:55.318337Z"}},"outputs":[{"name":"stdout","text":"Train Dataset Created!\nValidation Dataset Created!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2765993235.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mds_test_cifake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test_cifake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mds_test_ood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_ood_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ds_test_cifake' is not defined"],"ename":"NameError","evalue":"name 'ds_test_cifake' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"## 2.2 Data Exploration and Visualisation","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Show sample images from the training dataset\nWe visualize sample images by taking the first batch from the training dataset. Since the dataset is shuffled on loading, each batch is a random collection of images. From this batch, we display the first 9 images in a 3×3 grid along with their corresponding class labels (“real” or “fake”).\n\nWe do this to perform an initial quality check of the dataset: visual inspection allows us to confirm that the images were loaded correctly, that the labels correspond to the expected classes, and that there are no obvious issues such as corrupted files, incorrect preprocessing, or mislabeled images. Showing random samples also helps us get an intuitive understanding of what the model will see during training and whether the dataset contains sufficient visual variability for effective learning.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt         \nimport numpy as np            \nimport os   \n\nclass_names = train_dd.class_names     \nprint(\"Class names:\", class_names)      \n\nplt.figure(figsize=(10, 10))            \n\nfor images, labels in train_dd.take(1):  \n\n for i in range(9):                   \n        ax = plt.subplot(3, 3, i + 1)  \n        plt.imshow(images[i].numpy().astype(\"uint8\"))   \n        plt.title(class_names[labels[i]])              \n        plt.axis(\"off\")                                 \n\nplt.show()                             \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:43:51.974795Z","iopub.execute_input":"2025-12-11T09:43:51.975577Z","iopub.status.idle":"2025-12-11T09:43:51.991324Z","shell.execute_reply.started":"2025-12-11T09:43:51.975546Z","shell.execute_reply":"2025-12-11T09:43:51.990463Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1407365336.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class names:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dd' is not defined"],"ename":"NameError","evalue":"name 'train_dd' is not defined","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"### 2.2.2 Check class distribution\nWe assess the class distribution by counting how many images belong to each category (“real” and “fake”). A balanced dataset is important because severe class imbalance can bias the model toward predicting the majority class. By examining the distribution visually and numerically, we ensure that the model will be trained on approximately equal amounts of real and AI-generated images, reducing the risk of skewed learning or misclassification patterns.","metadata":{}},{"cell_type":"code","source":"train_real = len(os.listdir(os.path.join(deepdetect_train_dir, \"real\")))\ntrain_fake = len(os.listdir(os.path.join(deepdetect_train_dir, \"fake\")))\n\nprint(\"Number of training images:\")\nprint(\"Real:\", train_real)\nprint(\"Fake:\", train_fake)\n\nplt.figure(figsize=(6, 4))\nplt.bar([\"Real\", \"Fake\"], [train_real, train_fake], color=[\"blue\", \"orange\"])\nplt.title(\"Class Distribution in the Training Set\")\nplt.ylabel(\"Number of Images\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:44:04.802207Z","iopub.execute_input":"2025-12-11T09:44:04.802506Z","iopub.status.idle":"2025-12-11T09:44:04.923571Z","shell.execute_reply.started":"2025-12-11T09:44:04.802482Z","shell.execute_reply":"2025-12-11T09:44:04.923003Z"}},"outputs":[{"name":"stdout","text":"Number of training images:\nReal: 48815\nFake: 41594\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### 2.2.3 Inspect Image Shape and Label Format\nWe inspect the shape of one batch to verify that the images and labels are formatted as expected. The image batch typically has the shape (batch_size, height, width, channels) e.g., (32, 224, 224, 3), indicating 32 RGB images of size 224×224 pixels.\n\nThe label batch should have the shape (batch_size,), containing one integer label per image. Confirming these shapes helps ensure that the data pipeline is correctly configured before building the neural network model.","metadata":{}},{"cell_type":"code","source":"for batch_images, batch_labels in train_dd.take(1):\n    print(\"Image batch shape:\", batch_images.shape)\n    print(\"Label batch shape:\", batch_labels.shape)\n    print(\"Example label:\", batch_labels[0].numpy())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:44:12.463984Z","iopub.execute_input":"2025-12-09T16:44:12.464589Z","iopub.status.idle":"2025-12-09T16:44:12.56225Z","shell.execute_reply.started":"2025-12-09T16:44:12.464567Z","shell.execute_reply":"2025-12-09T16:44:12.561101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The batch shape (32, 224, 224, 3) confirms that images are correctly loaded in batches of 32, each resized to 224×224 pixels with three RGB channels. The label batch (32,) shows that each image has one corresponding class label. The example label 0/1 indicates that the labeling system is functioning as expected and correctly maps images to their respective classes. Overall, the data pipeline is properly structured for model training.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### 2.3.1 Data Augmentation\nWe apply data augmentation to artificially increase the variability of the training dataset and improve the model’s ability to generalize. The augmentation pipeline randomly flips, rotates, and zooms images during training, introducing meaningful variations that help the model become more robust to common transformations. This reduces overfitting and improves performance on unseen data.\n","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),   \n    tf.keras.layers.RandomRotation(0.1),        \n    tf.keras.layers.RandomZoom(0.1),            \n], name=\"data_augmentation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:44:19.754354Z","iopub.execute_input":"2025-12-09T16:44:19.754704Z","iopub.status.idle":"2025-12-09T16:44:19.767603Z","shell.execute_reply.started":"2025-12-09T16:44:19.754682Z","shell.execute_reply":"2025-12-09T16:44:19.766916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3.2 Visualizing Data Augmentation\nWe visualize a batch of augmented images to verify that the applied transformations (flipping, rotation, and zoom) behave as intended and produce realistic variations. This quick inspection ensures that augmentation does not distort the data in a harmful way and confirms that the model will receive meaningful, correctly transformed inputs during training.","metadata":{}},{"cell_type":"code","source":"for images, labels in train_dd.take(1):\n    augmented = data_augmentation(images)\n    break\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented[i].numpy().astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:44:24.466866Z","iopub.execute_input":"2025-12-11T09:44:24.467419Z","iopub.status.idle":"2025-12-11T09:44:24.48345Z","shell.execute_reply.started":"2025-12-11T09:44:24.467386Z","shell.execute_reply":"2025-12-11T09:44:24.482548Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1810677079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0maugmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dd' is not defined"],"ename":"NameError","evalue":"name 'train_dd' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"## 2.4 Neural Models","metadata":{}},{"cell_type":"markdown","source":"### 2.4.1 Custom CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Custom-made CNN classifier \ncustom_cnn = keras.Sequential([\n    # Input layer\n    layers.Input(shape=(224, 224, 3)),\n    \n    # First convolution + maxpool block (32 filters)\n    layers.Conv2D(filters=32, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Second convolution + maxpool block (64 filters)\n    layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Third convolution + maxpool block (128 filters)\n    layers.Conv2D(filters=128, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    #Fourth convolution + maxpool block (256 filters)\n    layers.Conv2D(filters=256, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),\n    # removed maxpool to preserve fine-grained details\n\n    # Global average pooling block\n    layers.GlobalAveragePooling2D(),\n\n    # Fully connected block\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(64,activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid'),\n    \n])\n\ncustom_cnn.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\ncustom_cnn.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:08:12.460986Z","iopub.execute_input":"2025-12-11T18:08:12.46129Z","iopub.status.idle":"2025-12-11T18:08:12.576444Z","shell.execute_reply.started":"2025-12-11T18:08:12.461268Z","shell.execute_reply":"2025-12-11T18:08:12.575926Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m433,601\u001b[0m (1.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">433,601</span> (1.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m432,641\u001b[0m (1.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">432,641</span> (1.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n</pre>\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### 2.4.2 Pre-trained CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**\n\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nIMG_SIZE = 224\nBATCH_SIZE = 32\n\n# Load pretrained EfficientNetB0\nbase_model = tf.keras.applications.EfficientNetB0(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\nbase_model.trainable = False  # freeze weights for transfer learning\n\n# Build model\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = tf.keras.applications.efficientnet.preprocess_input(inputs)\nx = base_model(x, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel_cnn = models.Model(inputs, outputs)\n\nmodel_cnn.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nmodel_cnn.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:07:44.495259Z","iopub.execute_input":"2025-12-11T18:07:44.495844Z","iopub.status.idle":"2025-12-11T18:07:45.883914Z","shell.execute_reply.started":"2025-12-11T18:07:44.495816Z","shell.execute_reply":"2025-12-11T18:07:45.883339Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### 2.4.3 Visual Transformer","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# 1️⃣ Imports & Setup\n# ------------------------------\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom transformers import ViTForImageClassification\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport os\n\n# Enable faster TF GPU memory allocation\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# ------------------------------\n# 2️⃣ Load Model & Optimize\n# ------------------------------\nmodel = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\",\n    num_labels=2,\n    ignore_mismatched_sizes=True\n).to(device)\n\n# --- SPEEDUP 1: Compile the model (PyTorch 2.0+) ---\n# This fuses layers and optimizes the computation graph.\n# If this errors (rare on old setups), comment it out.\ntry:\n    print(\"Compiling model with torch.compile...\")\n    model = torch.compile(model)\nexcept Exception as e:\n    print(f\"Skipping torch.compile (not supported): {e}\")\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# --- SPEEDUP 2: Mixed Precision Scaler ---\n# This manages the math for 16-bit training automatically.\nscaler = torch.amp.GradScaler('cuda')\n\n# ------------------------------\n# 3️⃣ Optimized Data Transfer\n# ------------------------------\ndef tf_to_torch_fast(images, labels):\n    # 1. Zero-copy from TF to Torch on CPU\n    images = torch.from_numpy(np.array(images)) \n    labels = torch.from_numpy(np.array(labels)).long().to(device)\n\n    # 2. Move raw uint8 to GPU (Small transfer)\n    images = images.to(device, non_blocking=True)\n\n    # 3. Process on GPU\n    images = images.permute(0, 3, 1, 2) # (B, H, W, C) -> (B, C, H, W)\n    images = images.float() / 255.0     # Normalize\n    \n    return images, labels\n\n# ------------------------------\n# 4️⃣ Training Loop with AMP\n# ------------------------------\n# Consider increasing Batch Size in your TF dataset creation if VRAM allows!\ntrain_dataset = tfds.as_numpy(ds_train_full) \n\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    print(f\"🚀 Starting Epoch {epoch+1} with Mixed Precision...\")\n    \n    for batch_idx, (images, labels) in enumerate(train_dataset):\n        images, labels = tf_to_torch_fast(images, labels)\n        \n        optimizer.zero_grad()\n        \n        # --- SPEEDUP 3: Automatic Mixed Precision Context ---\n        with torch.amp.autocast('cuda'):\n            outputs = model(images)\n            loss = criterion(outputs.logits, labels)\n        \n        # Scale the loss and step (handles FP16 gradients)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # Metrics\n        running_loss += loss.item() * labels.size(0)\n        preds = torch.argmax(outputs.logits, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n        if batch_idx % 100 == 0 and batch_idx > 0:\n            print(f\"  Batch {batch_idx}: Acc: {correct/total:.4f}\")\n\n    if total > 0:\n        epoch_loss = running_loss / total\n        epoch_acc = correct / total\n        print(f\"Epoch {epoch+1} Finished — Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:31:08.360548Z","iopub.execute_input":"2025-12-11T17:31:08.360873Z","iopub.status.idle":"2025-12-11T17:41:18.235412Z","shell.execute_reply.started":"2025-12-11T17:31:08.360823Z","shell.execute_reply":"2025-12-11T17:41:18.234234Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Compiling model with torch.compile...\n🚀 Starting Epoch 1 with Mixed Precision...\n","output_type":"stream"},{"name":"stderr","text":"W1211 17:31:30.814000 234 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n","output_type":"stream"},{"name":"stdout","text":"  Batch 100: Acc: 0.4907\n  Batch 200: Acc: 0.5039\n","output_type":"stream"},{"name":"stderr","text":"Invalid SOS parameters for sequential JPEG\n","output_type":"stream"},{"name":"stdout","text":"  Batch 300: Acc: 0.5076\n  Batch 400: Acc: 0.5087\n  Batch 500: Acc: 0.5208\n  Batch 600: Acc: 0.5366\n  Batch 700: Acc: 0.5501\n  Batch 800: Acc: 0.5554\n  Batch 900: Acc: 0.5651\n  Batch 1000: Acc: 0.5750\n  Batch 1100: Acc: 0.5844\n  Batch 1200: Acc: 0.5934\n  Batch 1300: Acc: 0.6016\n  Batch 1400: Acc: 0.6109\n  Batch 1500: Acc: 0.6211\n  Batch 1600: Acc: 0.6305\n  Batch 1700: Acc: 0.6395\n  Batch 1800: Acc: 0.6489\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_234/2227588140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🚀 Starting Epoch {epoch+1} with Mixed Precision...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_to_torch_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_datasets/core/dataset_utils.py\u001b[0m in \u001b[0;36m_eager_dataset_iterator\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_eager_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNumpyElem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_elem_to_numpy_eager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:4 transformation with iterator: Iterator::Root::ParallelMapV2::Prefetch::BatchV2::Shuffle::ParallelMapV2::DirectedInterleave[0]::Prefetch::ParallelMapV2: /kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/3509.jpg; Unknown error 512\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "],"ename":"UnknownError","evalue":"{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:4 transformation with iterator: Iterator::Root::ParallelMapV2::Prefetch::BatchV2::Shuffle::ParallelMapV2::DirectedInterleave[0]::Prefetch::ParallelMapV2: /kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/3509.jpg; Unknown error 512\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## 2.5 Loss and Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"## 2.6 Fitting\n","metadata":{}},{"cell_type":"markdown","source":"### 2.6.1 Early Stopping","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',        # Watch validation loss (metric of truth)\n    patience=3,                # Wait 3 epochs before stopping (gives it a chance to recover)\n    restore_best_weights=True, # CRITICAL: Go back to the best weights, not the last ones\n    verbose=1                  # Print a message when it triggers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:23:05.858747Z","iopub.execute_input":"2025-12-11T18:23:05.859238Z","iopub.status.idle":"2025-12-11T18:23:05.864792Z","shell.execute_reply.started":"2025-12-11T18:23:05.859213Z","shell.execute_reply":"2025-12-11T18:23:05.864182Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Function for training, predicting and cleaning (llm)","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport tensorflow as tf\n\n# Initialize the global results dictionary if it doesn't exist\nif 'all_results' not in globals():\n    all_results = {}\n\n# Define your test sets map\ntest_sets = {\n    \"CIFAKE_Test\": ds_test_cifake,\n    \"OOD_Test\":   ds_test_ood\n}\n\ndef train_eval_clean(model, model_name, train_ds, val_ds, test_sets, epochs=30):\n    \"\"\"\n    1. Trains the model.\n    2. Evaluates on all test/OOD sets.\n    3. Saves results to global 'all_results'.\n    4. Saves model to disk.\n    5. Deletes model and clears VRAM.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"🚀 STARTING PIPELINE FOR: {model_name}\")\n    print(f\"{'='*60}\")\n\n    # ---------------------------------------------------------\n    # 1. TRAIN\n    # ---------------------------------------------------------\n    print(f\"📉 Training {model_name}...\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        verbose=1,\n        callbacks=[early_stopping] # Assumes 'early_stopping' is defined globally\n    )\n\n    # ---------------------------------------------------------\n    # 2. EVALUATE\n    # ---------------------------------------------------------\n    print(f\"\\n🔍 Evaluating {model_name} on {len(test_sets)} datasets...\")\n    \n    # Create entry in global results dict\n    all_results[model_name] = {}\n\n    for ds_name, ds in test_sets.items():\n        print(f\"   • Predicting on {ds_name}...\")\n        \n        # A. Predict\n        preds = model.predict(ds, verbose=0) # verbose=0 to keep logs clean\n        \n        # Handle shapes: (N, 2) -> (N,) or (N, 1) -> (N,)\n        if preds.shape[-1] > 1:\n            y_pred = preds[:, 1] \n        else:\n            y_pred = preds.flatten()\n            \n        # B. Get True Labels\n        y_true = np.concatenate([y for x, y in ds], axis=0)\n        \n        # C. Store\n        all_results[model_name][ds_name] = {\n            'y_pred': y_pred,\n            'y_true': y_true\n        }\n\n    # ---------------------------------------------------------\n    # 3. SAVE TO DISK\n    # ---------------------------------------------------------\n    filename = f\"{model_name}_final.keras\"\n    model.save(filename)\n    print(f\"\\n💾 Model saved to: {filename}\")\n\n    # ---------------------------------------------------------\n    # 4. CLEANUP (The most important part)\n    # ---------------------------------------------------------\n    print(f\"🧹 Scrubbing VRAM...\")\n    del model\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\"✨ {model_name} pipeline complete. GPU is ready for next model.\\n\")\n    \n    return history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6.2 Custom CNN","metadata":{}},{"cell_type":"code","source":"hist_custom = train_eval_clean(\n    custom_cnn, \n    \"Custom_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=2 # Change to 30 for real run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:23:08.276053Z","iopub.execute_input":"2025-12-11T18:23:08.27654Z","iopub.status.idle":"2025-12-11T18:23:14.611782Z","shell.execute_reply.started":"2025-12-11T18:23:08.276517Z","shell.execute_reply":"2025-12-11T18:23:14.610848Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n     47/Unknown \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.7805 - loss: 0.4269","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/111594199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the custom CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history_custom_check = custom_cnn.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mds_train_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_val_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"### 2.6.3 Pre-trained CNN","metadata":{}},{"cell_type":"code","source":"hist_cnn = train_eval_clean(\n    model_cnn, \n    \"Pretrained_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=2 # Change to 30 for real run\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:23:18.028295Z","iopub.execute_input":"2025-12-11T18:23:18.028576Z","iopub.status.idle":"2025-12-11T18:23:29.260752Z","shell.execute_reply.started":"2025-12-11T18:23:18.028557Z","shell.execute_reply":"2025-12-11T18:23:29.259776Z"}},"outputs":[{"name":"stdout","text":"    150/Unknown \u001b[1m11s\u001b[0m 70ms/step - accuracy: 0.7414 - loss: 0.5128","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3056162941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the pre-trained CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model_cnn.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mds_train_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_val_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"### 2.6.3 Visual Transformer","metadata":{}},{"cell_type":"markdown","source":"# 3. Results","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Hold-out set performance ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Out-of-distribution set performance","metadata":{}},{"cell_type":"markdown","source":"## 3.3 Performance on custom images ","metadata":{}},{"cell_type":"code","source":"# use a pic of johannes and a pic of ai generated johannes to see if it predicts correctly ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Conclusion/Discussion","metadata":{}},{"cell_type":"markdown","source":"# 5. References","metadata":{}},{"cell_type":"markdown","source":"# 6. Division of Labour","metadata":{}}]}