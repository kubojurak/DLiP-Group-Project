{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726,"isSourceIdPinned":false},{"sourceId":11658923,"sourceType":"datasetVersion","datasetId":7316517,"isSourceIdPinned":false}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/annaattuch/dlip-notebook-group-4?scriptVersionId=284928810\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background-color:#5F9EA0; \n            color:white; \n            padding:15px; \n            border-radius:10px; \n            text-align:center; \n            font-size:30px; \n            font-weight:bold;\">\n    Detecting Deep Fakes - A Deep Learning Computer Vision Task<br>\n    <span style=\"font-size:20px; font-weight:normal;\">\n        Hairy Feet: Anna, Jacob, Johannes\n    </span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n    1. Introduction: Project Overview & Data \n</h1","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nIn this project, we build a deep learning computer vision model to distinguish between real human faces, and AI-generated (deepfake) face images. \n\nWe use the **DeepDetect-2025** dataset from Kaggle, which contains over 100k labeled images of faces, split into two classes: real and fake. The goal is to train a binary classifier that can automatically detect whether an image is genuine or AI generated.","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n     2. Importing and Preparing Data\n</h1","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download DeepDetect-2025 dataset\npath = kagglehub.dataset_download(\"ayushmandatta1/deepdetect-2025\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download CIFAKE dataset (for later use)\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:24:20.129446Z","iopub.execute_input":"2025-12-09T12:24:20.129792Z","iopub.status.idle":"2025-12-09T12:24:20.503368Z","shell.execute_reply.started":"2025-12-09T12:24:20.129764Z","shell.execute_reply":"2025-12-09T12:24:20.502293Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/deepdetect-2025\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n\nThe DeepDetect-2025 dataset is organized into separate \"train\" and \"test\" folders, each containing two subfolders:\n\n- \"real\" = real human face images  \n- \"fake\" = AI-generated (deepfake) face images  \n\nWe will:\n- Set a fixed image size and batch size\n- Point TensorFlow to the \"train\" and \"test\" directories\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\ndeepdetect_train_dir = \"/kaggle/input/deepdetect-2025/ddata/train\"\ndeepdetect_test_dir  = \"/kaggle/input/deepdetect-2025/ddata/test\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:24:25.362161Z","iopub.execute_input":"2025-12-09T12:24:25.362489Z","iopub.status.idle":"2025-12-09T12:24:25.36833Z","shell.execute_reply.started":"2025-12-09T12:24:25.362463Z","shell.execute_reply":"2025-12-09T12:24:25.367241Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"\n<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nNow we create the training, validation and test sets. \nWe use tf.keras.utils.image_dataset_from_directory to:\n\n- Load images from the \"train\"  directory\n- Automatically split the training data into:\n  - **80% training**\n  - **20% validation**\n\nThe images in the \"test\" directory are used as a separate held-out test set that we will only use for final evaluation.","metadata":{}},{"cell_type":"code","source":"\ntrain_dd = tf.keras.utils.image_dataset_from_directory(\n    deepdetect_train_dir,\n    validation_split=0.2,      \n    subset=\"training\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE\n)\n\nval_dd = tf.keras.utils.image_dataset_from_directory(\n    deepdetect_train_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE\n)\n\ntest_dd = tf.keras.utils.image_dataset_from_directory(\n    deepdetect_test_dir,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False    \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:24:27.91584Z","iopub.execute_input":"2025-12-09T12:24:27.916866Z"}},"outputs":[{"name":"stdout","text":"Found 90409 files belonging to 2 classes.\nUsing 72328 files for training.\n","output_type":"stream"},{"name":"stderr","text":"2025-12-09 12:25:17.861353: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n     3.Data Exploration and Visualization\n</h1","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Show sample images from the training dataset\nWe visualize sample images by taking the first batch from the training dataset. Since the dataset is shuffled on loading, each batch is a random collection of images. From this batch, we display the first 9 images in a 3×3 grid along with their corresponding class labels (“real” or “fake”).\n\nWe do this to perform an initial quality check of the dataset: visual inspection allows us to confirm that the images were loaded correctly, that the labels correspond to the expected classes, and that there are no obvious issues such as corrupted files, incorrect preprocessing, or mislabeled images. Showing random samples also helps us get an intuitive understanding of what the model will see during training and whether the dataset contains sufficient visual variability for effective learning.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt         \nimport numpy as np            \nimport os   \n\nclass_names = train_dd.class_names     \nprint(\"Class names:\", class_names)      \n\nplt.figure(figsize=(10, 10))            \n\nfor images, labels in train_dd.take(1):  \n\n for i in range(9):                   \n        ax = plt.subplot(3, 3, i + 1)  \n        plt.imshow(images[i].numpy().astype(\"uint8\"))   \n        plt.title(class_names[labels[i]])              \n        plt.axis(\"off\")                                 \n\nplt.show()                             \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Check class distribution\nWe assess the class distribution by counting how many images belong to each category (“real” and “fake”). A balanced dataset is important because severe class imbalance can bias the model toward predicting the majority class. By examining the distribution visually and numerically, we ensure that the model will be trained on approximately equal amounts of real and AI-generated images, reducing the risk of skewed learning or misclassification patterns.","metadata":{}},{"cell_type":"code","source":"train_real = len(os.listdir(os.path.join(deepdetect_train_dir, \"real\")))\ntrain_fake = len(os.listdir(os.path.join(deepdetect_train_dir, \"fake\")))\n\nprint(\"Number of training images:\")\nprint(\"Real:\", train_real)\nprint(\"Fake:\", train_fake)\n\nplt.figure(figsize=(6, 4))\nplt.bar([\"Real\", \"Fake\"], [train_real, train_fake], color=[\"blue\", \"orange\"])\nplt.title(\"Class Distribution in the Training Set\")\nplt.ylabel(\"Number of Images\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Inspect Image Shape and Label Format\nWe inspect the shape of one batch to verify that the images and labels are formatted as expected. The image batch typically has the shape (batch_size, height, width, channels) e.g., (32, 224, 224, 3), indicating 32 RGB images of size 224×224 pixels.\n\nThe label batch should have the shape (batch_size,), containing one integer label per image. Confirming these shapes helps ensure that the data pipeline is correctly configured before building the neural network model.","metadata":{}},{"cell_type":"code","source":"for batch_images, batch_labels in train_dd.take(1):\n    print(\"Image batch shape:\", batch_images.shape)\n    print(\"Label batch shape:\", batch_labels.shape)\n    print(\"Example label:\", batch_labels[0].numpy())\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The batch shape (32, 224, 224, 3) confirms that images are correctly loaded in batches of 32, each resized to 224×224 pixels with three RGB channels. The label batch (32,) shows that each image has one corresponding class label. The example label 0/1 indicates that the labeling system is functioning as expected and correctly maps images to their respective classes. Overall, the data pipeline is properly structured for model training.","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n     4.Modeling\n</h1","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Data Augmentation\nWe apply data augmentation to artificially increase the variability of the training dataset and improve the model’s ability to generalize. The augmentation pipeline randomly flips, rotates, and zooms images during training, introducing meaningful variations that help the model become more robust to common transformations. This reduces overfitting and improves performance on unseen data.\n","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),   \n    tf.keras.layers.RandomRotation(0.1),        \n    tf.keras.layers.RandomZoom(0.1),            \n], name=\"data_augmentation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 Visualizing Data Augmentation\nWe visualize a batch of augmented images to verify that the applied transformations (flipping, rotation, and zoom) behave as intended and produce realistic variations. This quick inspection ensures that augmentation does not distort the data in a harmful way and confirms that the model will receive meaningful, correctly transformed inputs during training.","metadata":{}},{"cell_type":"code","source":"for images, labels in train_dd.take(1):\n    augmented = data_augmentation(images)\n    break\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented[i].numpy().astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}