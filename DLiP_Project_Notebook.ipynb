{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726,"isSourceIdPinned":false},{"sourceId":9251202,"sourceType":"datasetVersion","datasetId":5596873},{"sourceId":11473304,"sourceType":"datasetVersion","datasetId":7190440},{"sourceId":11973689,"sourceType":"datasetVersion","datasetId":7180950}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#5F9EA0; \n            color:white; \n            padding:15px; \n            border-radius:10px; \n            text-align:center; \n            font-size:30px; \n            font-weight:bold;\">\n    Detecting Deep Fakes - A Deep Learning Computer Vision Task<br>\n    <span style=\"font-size:20px; font-weight:normal;\">\n        Hairy Feet: Anna, Jacob, Johannes\n    </span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n    1. Introduction: Project Overview & Data \n</h1","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nIn this project, we build a deep learning computer vision model to distinguish between real human faces, and AI-generated (deepfake) face images. \n\nWe use the **DeepDetect-2025** dataset from Kaggle, which contains over 100k labeled images of faces, split into two classes: real and fake. The goal is to train a binary classifier that can automatically detect whether an image is genuine or AI generated.","metadata":{}},{"cell_type":"markdown","source":"# 2. Methods","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Importing and Preparing Data","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download CIFAKE dataset (for later use)\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download Deepfake 20K\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download Human-faces dataset\npath = kagglehub.dataset_download(\"/kaggle/input/human-faces-dataset\")\n\n# Download 200K dataset\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:49:21.822971Z","iopub.execute_input":"2025-12-11T09:49:21.823259Z","iopub.status.idle":"2025-12-11T09:49:22.808956Z","shell.execute_reply.started":"2025-12-11T09:49:21.823237Z","shell.execute_reply":"2025-12-11T09:49:22.808399Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/deepdetect-2025\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n\nThe DeepDetect-2025 dataset is organized into separate \"train\" and \"test\" folders, each containing two subfolders:\n\n- \"real\" = real human face images  \n- \"fake\" = AI-generated (deepfake) face images  \n\nWe will:\n- Set a fixed image size and batch size\n- Point TensorFlow to the \"train\" and \"test\" directories\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndeepdetect_train_dir = \"/kaggle/input/deepdetect-2025/ddata/train\"\ndeepdetect_test_dir  = \"/kaggle/input/deepdetect-2025/ddata/test\"\n\ncifake_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\ncifake_test_dir  = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test\"\n\ndf20k_dir = \"/kaggle/input/deepfake-vs-real-20k/Deep-vs-Real\"\n\nhf_dir = \"/kaggle/input/human-faces-dataset/Human Faces Dataset\"\n\nds200k_dir = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/my_real_vs_ai_dataset/my_real_vs_ai_dataset\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:21:38.743998Z","iopub.execute_input":"2025-12-12T09:21:38.744810Z","iopub.status.idle":"2025-12-12T09:21:38.748847Z","shell.execute_reply.started":"2025-12-12T09:21:38.744781Z","shell.execute_reply":"2025-12-12T09:21:38.748022Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"\n<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nNow we create the training, validation and test sets. \nWe use tf.keras.utils.image_dataset_from_directory to:\n\n- Load images from the \"train\"  directory\n- Automatically split the training data into:\n  - **80% training**\n  - **20% validation**\n\nThe images in the \"test\" directory are used as a separate held-out test set that we will only use for final evaluation.","metadata":{}},{"cell_type":"code","source":"# Check the shape of one element\nprint(\"Train Spec:\", ds_train_cifake.element_spec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:40:13.153705Z","iopub.execute_input":"2025-12-12T09:40:13.154020Z","iopub.status.idle":"2025-12-12T09:40:13.158722Z","shell.execute_reply.started":"2025-12-12T09:40:13.153999Z","shell.execute_reply":"2025-12-12T09:40:13.157904Z"}},"outputs":[{"name":"stdout","text":"Train Spec: (TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Helper function for getting dataset\ndef create_ds(directory, subset, labels, IMG_SIZE=(224,224), BATCH_SIZE=None, shuffle=True):\n    ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        validation_split=0.2 if subset else None, # Only split if subset is asked for\n        subset=subset,\n        seed=2025,\n        class_names=labels,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        label_mode='int',\n        shuffle=shuffle \n    )\n    return ds\n\n# --- CIFAKE ---\nds_train_cifake = create_ds(cifake_train_dir, 'training', ['FAKE', 'REAL'])\nds_val_cifake   = create_ds(cifake_train_dir, 'validation', ['FAKE', 'REAL'])\n\nds_train_cifake = ds_train_cifake.apply(tf.data.experimental.ignore_errors()) # Because it has corrupted files that crash the system otherwise\n\n#-- 200K Dataset--\nds_train_200k = create_ds(ds200k_dir, 'training', ['ai_images', 'real'])\nds_val_200k   = create_ds(ds200k_dir, 'validation', ['ai_images', 'real'])\n\n# CIFAKE hold-out set\nds_test_cifake = create_ds(cifake_test_dir, None, ['FAKE', 'REAL'], shuffle = False)\n\n# Out-of-Distribution (face) sets\n\n# --- Deepfake 20k ---\nds_ood_df20k = create_ds(df20k_dir, None, ['Deepfake', 'Real'], shuffle = False)\n\n# --- HF Dataset ---\nds_ood_hf = create_ds(hf_dir, None , ['AI-Generated Images', 'Real Images'], shuffle = False)\n\n# 2. Concatenate them (Append HF to the end of DF20k)\nds_ood_combined = ds_ood_df20k.concatenate(ds_ood_hf)\n\nprint(\"Datasets created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:38:05.546023Z","iopub.execute_input":"2025-12-12T09:38:05.546708Z","iopub.status.idle":"2025-12-12T09:38:12.659182Z","shell.execute_reply.started":"2025-12-12T09:38:05.546682Z","shell.execute_reply":"2025-12-12T09:38:12.657874Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1887301689.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- CIFAKE ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mds_train_cifake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifake_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mds_val_cifake\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcreate_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifake_train_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1887301689.py\u001b[0m in \u001b[0;36mcreate_ds\u001b[0;34m(directory, subset, labels, IMG_SIZE, BATCH_SIZE, shuffle)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Helper function for getting dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Only split if subset is asked for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Define a helper to enforce shapes\ndef force_shape(image, label):\n    # Explicitly set the shape. This converts RaggedTensors to Dense Tensors.\n    image = tf.ensure_shape(image, (224, 224, 3))\n    label = tf.ensure_shape(label, ())\n    return image, label\n\n# Full dataset function\ndef ds_full(datasets, weights, batch_size = 64):\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    ds_full = tf.data.Dataset.sample_from_datasets(\n        datasets,\n        weights=weights, # Optional: Balance the datasets if one is huge\n        stop_on_empty_dataset=False\n    )\n \n    ds_full = ds_full.unbatch() # Safety check to stop code from crashing if cell is run twice\n    \n    ds_full = ds_full.map(force_shape, num_parallel_calls=tf.data.AUTOTUNE)\n\n    ds_full = ds_full.shuffle(buffer_size=1000)\n    ds_full = ds_full.batch(batch_size)\n    ds_full = ds_full.prefetch(tf.data.AUTOTUNE)\n\n    return ds_full\n\n# Test set function\ndef prepare_test_set(ds, batch_size=64):\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    ds = ds.unbatch() # Safety check to stop code from crashing if cell is run twice\n    \n    ds = ds.map(force_shape, num_parallel_calls=AUTOTUNE)\n    \n    ds = ds.batch(batch_size)\n    \n    ds = ds.prefetch(AUTOTUNE)\n    \n    # NOTE: No shuffle(), No sample_from_datasets()\n    return ds\n\n# Full training set\nds_train_full = ds_full([ds_train_cifake, ds_train_200k], [1.0, 1.0])\nprint(\"Train Dataset Created!\")\n\n# Full validation set\nds_val_full = ds_full([ds_val_cifake, ds_val_200k], [1.0, 1.0])\nprint(\"Validation Dataset Created!\")\n\n# Test set\nds_test_cifake = prepare_test_set(ds_test_cifake)\n\nds_test_ood = prepare_test_set(ds_ood_combined)\nprint(\"Test Datasets Created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:40:17.113958Z","iopub.execute_input":"2025-12-12T09:40:17.114848Z","iopub.status.idle":"2025-12-12T09:40:17.210075Z","shell.execute_reply.started":"2025-12-12T09:40:17.114782Z","shell.execute_reply":"2025-12-12T09:40:17.208818Z"}},"outputs":[{"name":"stdout","text":"Train Dataset Created!\nValidation Dataset Created!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/697859316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mds_test_cifake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test_cifake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mds_test_ood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_ood_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/697859316.py\u001b[0m in \u001b[0;36mprepare_test_set\u001b[0;34m(ds, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mAUTOTUNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_fileofwatfc2.py\u001b[0m in \u001b[0;36mtf__force_shape\u001b[0;34m(image, label)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_47/3987693300.py\", line 7, in force_shape  *\n        image = tf.ensure_shape(image, (224, 224, 3))\n\n    ValueError: Shape must be rank 3 but is rank 4 for '{{node EnsureShape}} = EnsureShape[T=DT_FLOAT, shape=[224,224,3]](args_0)' with input shapes: [?,224,224,3].\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/tmp/ipykernel_47/3987693300.py\", line 7, in force_shape  *\n        image = tf.ensure_shape(image, (224, 224, 3))\n\n    ValueError: Shape must be rank 3 but is rank 4 for '{{node EnsureShape}} = EnsureShape[T=DT_FLOAT, shape=[224,224,3]](args_0)' with input shapes: [?,224,224,3].\n","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"## 2.2 Data Exploration and Visualisation","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Show sample images from the training dataset\nWe visualize sample images by taking the first batch from the training dataset. Since the dataset is shuffled on loading, each batch is a random collection of images. From this batch, we display the first 9 images in a 33 grid along with their corresponding class labels (real or fake).\n\nWe do this to perform an initial quality check of the dataset: visual inspection allows us to confirm that the images were loaded correctly, that the labels correspond to the expected classes, and that there are no obvious issues such as corrupted files, incorrect preprocessing, or mislabeled images. Showing random samples also helps us get an intuitive understanding of what the model will see during training and whether the dataset contains sufficient visual variability for effective learning.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.2 Check class distribution\nWe assess the class distribution by counting how many images belong to each category (real and fake). A balanced dataset is important because severe class imbalance can bias the model toward predicting the majority class. By examining the distribution visually and numerically, we ensure that the model will be trained on approximately equal amounts of real and AI-generated images, reducing the risk of skewed learning or misclassification patterns.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.3 Inspect Image Shape and Label Format\nWe inspect the shape of one batch to verify that the images and labels are formatted as expected. The image batch typically has the shape (batch_size, height, width, channels) e.g., (32, 224, 224, 3), indicating 32 RGB images of size 224224 pixels.\n\nThe label batch should have the shape (batch_size,), containing one integer label per image. Confirming these shapes helps ensure that the data pipeline is correctly configured before building the neural network model.","metadata":{}},{"cell_type":"markdown","source":"The batch shape (32, 224, 224, 3) confirms that images are correctly loaded in batches of 32, each resized to 224224 pixels with three RGB channels. The label batch (32,) shows that each image has one corresponding class label. The example label 0/1 indicates that the labeling system is functioning as expected and correctly maps images to their respective classes. Overall, the data pipeline is properly structured for model training.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### 2.3.1 Data Augmentation\nWe apply data augmentation to artificially increase the variability of the training dataset and improve the models ability to generalize. The augmentation pipeline randomly flips, rotates, and zooms images during training, introducing meaningful variations that help the model become more robust to common transformations. This reduces overfitting and improves performance on unseen data.\n","metadata":{}},{"cell_type":"markdown","source":"### 2.3.2 Visualizing Data Augmentation\nWe visualize a batch of augmented images to verify that the applied transformations (flipping, rotation, and zoom) behave as intended and produce realistic variations. This quick inspection ensures that augmentation does not distort the data in a harmful way and confirms that the model will receive meaningful, correctly transformed inputs during training.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Neural Models","metadata":{}},{"cell_type":"markdown","source":"### 2.4.1 Custom CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Custom-made CNN classifier \ncustom_cnn = keras.Sequential([\n    # Input layer\n    layers.Input(shape=(224, 224, 3)),\n    \n    # First convolution + maxpool block (32 filters)\n    layers.Conv2D(filters=32, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Second convolution + maxpool block (64 filters)\n    layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Third convolution + maxpool block (128 filters)\n    layers.Conv2D(filters=128, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    #Fourth convolution + maxpool block (256 filters)\n    layers.Conv2D(filters=256, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),\n    # removed maxpool to preserve fine-grained details\n\n    # Global average pooling block\n    layers.GlobalAveragePooling2D(),\n\n    # Fully connected block\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(64,activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid'),\n    \n])\n\ncustom_cnn.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # Slower, more precise training\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\ncustom_cnn.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:45:05.773758Z","iopub.execute_input":"2025-12-12T09:45:05.774416Z","iopub.status.idle":"2025-12-12T09:45:06.716539Z","shell.execute_reply.started":"2025-12-12T09:45:05.774391Z","shell.execute_reply":"2025-12-12T09:45:06.715723Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n conv2d (\u001b[38;5;33mConv2D\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m896\u001b[0m \n\n batch_normalization              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m128\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n\n activation (\u001b[38;5;33mActivation\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m0\u001b[0m \n\n max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m0\u001b[0m \n\n conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)           \u001b[38;5;34m18,496\u001b[0m \n\n batch_normalization_1            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m256\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n\n activation_1 (\u001b[38;5;33mActivation\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)                \u001b[38;5;34m0\u001b[0m \n\n max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n\n conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m73,856\u001b[0m \n\n batch_normalization_2            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m512\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n\n activation_2 (\u001b[38;5;33mActivation\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n\n max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n\n conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)           \u001b[38;5;34m295,168\u001b[0m \n\n batch_normalization_3            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m1,024\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n\n activation_3 (\u001b[38;5;33mActivation\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n\n global_average_pooling2d         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n\n dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m32,896\u001b[0m \n\n dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m8,256\u001b[0m \n\n dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n\n dense_2 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m2,080\u001b[0m \n\n dropout_2 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n\n dense_3 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m33\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> \n\n batch_normalization              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n\n activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> \n\n batch_normalization_1            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n\n activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> \n\n batch_normalization_2            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n\n activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> \n\n batch_normalization_3            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n\n activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n global_average_pooling2d         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n\n dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n\n dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \n\n dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n\n dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m433,601\u001b[0m (1.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">433,601</span> (1.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m432,641\u001b[0m (1.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">432,641</span> (1.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n</pre>\n"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### 2.4.2 Pre-trained CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**\n\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nIMG_SIZE = 224\nBATCH_SIZE = 32\n\n# Load pretrained EfficientNetB0\nbase_model = tf.keras.applications.EfficientNetB0(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\nbase_model.trainable = False  # freeze weights for transfer learning\n\n# Build model\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = tf.keras.applications.efficientnet.preprocess_input(inputs)\nx = base_model(x, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel_cnn = models.Model(inputs, outputs)\n\nmodel_cnn.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_cnn.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:45:10.898868Z","iopub.execute_input":"2025-12-12T09:45:10.899193Z","iopub.status.idle":"2025-12-12T09:45:12.749729Z","shell.execute_reply.started":"2025-12-12T09:45:10.899171Z","shell.execute_reply":"2025-12-12T09:45:12.749114Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n\n efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          \u001b[38;5;34m4,049,571\u001b[0m \n\n global_average_pooling2d_1       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n\n dropout_3 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n\n dense_4 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                       \u001b[38;5;34m1,281\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> \n\n global_average_pooling2d_1       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n\n dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"### 2.4.3 Visual Transformer","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# 1 Imports & Setup\n# ------------------------------\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom transformers import ViTForImageClassification\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm  # Keras-style progress bar\nimport copy\nimport os\n\n# Enable faster TF GPU memory allocation\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nclass ViTClassifier:\n    def __init__(self, model_name=\"google/vit-base-patch16-224\", num_labels=2, learning_rate=2e-5):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\" Initializing ViTClassifier on {self.device}...\")\n\n        # 1. Load Model\n        self.model = ViTForImageClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels,\n            ignore_mismatched_sizes=True\n        ).to(self.device)\n\n        # ---------------------------------------------------------\n        #  FREEZING \n        # ---------------------------------------------------------\n        # Loop through the base model (self.model.vit) and turn off gradients\n        for param in self.model.vit.parameters():\n            param.requires_grad = False\n            \n        # Ensure the Classifier Head (self.model.classifier) is OPEN for training\n        for param in self.model.classifier.parameters():\n            param.requires_grad = True\n        # ---------------------------------------------------------\n\n        # 2. Compile (Optional Speedup)\n        try:\n            self.model = torch.compile(self.model)\n        except:\n            pass\n\n        # 3. Optimization (Only optimize parameters that require gradients)\n        # We filter the parameters to ensure the optimizer doesn't track frozen ones\n        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n        self.optimizer = AdamW(trainable_params, lr=learning_rate)\n        \n        self.criterion = nn.CrossEntropyLoss()\n        self.scaler = torch.amp.GradScaler('cuda')\n\n        self.history = {\n            'loss': [], 'accuracy': [],\n            'val_loss': [], 'val_accuracy': []\n        }\n\n    def _tf_to_torch_fast(self, images, labels):\n        \"\"\"\n        Internal helper: Optimized data transfer from TF (CPU) -> PyTorch (GPU).\n        Expects images to be uint8 [0, 255].\n        \"\"\"\n        # Zero-copy transfer to CPU tensor\n        images = torch.from_numpy(np.array(images)) \n        labels = torch.from_numpy(np.array(labels)).long().to(self.device)\n\n        # Move to GPU (non_blocking allows async transfer)\n        images = images.to(self.device, non_blocking=True)\n\n        # Permute (Batch, Height, Width, Channel) -> (Batch, Channel, Height, Width)\n        # And Normalize [0, 255] -> [0.0, 1.0]\n        images = images.permute(0, 3, 1, 2).float() / 255.0\n        \n        return images, labels\n\n    def _validate(self, val_loader):\n        \"\"\"Internal helper for validation loop\"\"\"\n        self.model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = self._tf_to_torch_fast(images, labels)\n                \n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(images)\n                    loss = self.criterion(outputs.logits, labels)\n                \n                val_loss += loss.item() * labels.size(0)\n                preds = torch.argmax(outputs.logits, dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        \n        # Avoid division by zero\n        if total == 0: return 0.0, 0.0\n        return val_loss / total, correct / total\n\n    def train(self, train_ds, val_ds, epochs=10, patience=3):\n        \"\"\"\n        Main training loop with:\n         - TQDM Progress Bar (Keras style)\n         - Early Stopping\n         - History tracking\n        \"\"\"\n        # Convert TF datasets to numpy iterables\n        # We try to get the length for the progress bar if possible\n        try:\n            n_batches = len(train_ds)\n        except:\n            n_batches = None\n\n        train_loader = tfds.as_numpy(train_ds)\n        val_loader = tfds.as_numpy(val_ds)\n        \n        best_val_loss = float('inf')\n        patience_counter = 0\n        best_model_weights = None\n        \n        print(f\"\\n Starting Training (Max Epochs: {epochs}, Patience: {patience})\")\n\n        for epoch in range(epochs):\n            self.model.train()\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            # --- Progress Bar ---\n            with tqdm(train_loader, total=n_batches, unit=\"batch\", leave=True) as pbar:\n                pbar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n                \n                for batch_idx, (images, labels) in enumerate(pbar):\n                    images, labels = self._tf_to_torch_fast(images, labels)\n                    \n                    self.optimizer.zero_grad()\n                    \n                    # Mixed Precision Forward\n                    with torch.amp.autocast('cuda'):\n                        outputs = self.model(images)\n                        loss = self.criterion(outputs.logits, labels)\n                    \n                    # Mixed Precision Backward\n                    self.scaler.scale(loss).backward()\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n\n                    # Metrics\n                    batch_size = labels.size(0)\n                    running_loss += loss.item() * batch_size\n                    \n                    preds = torch.argmax(outputs.logits, dim=1)\n                    batch_correct = (preds == labels).sum().item()\n                    correct += batch_correct\n                    total += batch_size\n                    \n                    # Live Update of Bar\n                    current_acc = correct / total\n                    current_loss = running_loss / total\n                    pbar.set_postfix({\"loss\": f\"{current_loss:.4f}\", \"acc\": f\"{current_acc:.4f}\"})\n\n            # --- Validation & History ---\n            val_loss, val_acc = self._validate(val_loader)\n            \n            self.history['loss'].append(current_loss)\n            self.history['accuracy'].append(current_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_accuracy'].append(val_acc)\n            \n            print(f\"    Validation - loss: {val_loss:.4f} - acc: {val_acc:.4f}\")\n\n            # --- Early Stopping ---\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                best_model_weights = copy.deepcopy(self.model.state_dict())\n                # print(\"     Saved Best Model\")\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"\\n Early stopping triggered! Restoring best weights.\")\n                    self.model.load_state_dict(best_model_weights)\n                    break\n        \n        # Restore best weights if finished naturally\n        if best_model_weights is not None and patience_counter < patience:\n             self.model.load_state_dict(best_model_weights)\n\n    def plot_history(self):\n        \"\"\"Plots accuracy and loss graphs similar to Keras.\"\"\"\n        if not self.history['loss']:\n            print(\"No training history to plot.\")\n            return\n\n        epochs_range = range(1, len(self.history['loss']) + 1)\n        \n        plt.figure(figsize=(12, 5))\n\n        # Plot Loss\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs_range, self.history['loss'], 'bo-', label='Training Loss')\n        plt.plot(epochs_range, self.history['val_loss'], 'r-', label='Validation Loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n\n        # Plot Accuracy\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs_range, self.history['accuracy'], 'bo-', label='Training Acc')\n        plt.plot(epochs_range, self.history['val_accuracy'], 'r-', label='Validation Acc')\n        plt.title('Training and Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def predict(self, tf_dataset):\n        \"\"\"Runs inference on a dataset and returns (y_true, y_pred_prob)\"\"\"\n        loader = tfds.as_numpy(tf_dataset)\n        self.model.eval()\n        \n        all_preds = []\n        all_labels = []\n        \n        print(f\" Running Prediction...\")\n        with torch.no_grad():\n            for images, labels in tqdm(loader, unit=\"batch\"):\n                images, labels = self._tf_to_torch_fast(images, labels)\n                \n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(images)\n                    \n                # Get probabilities (softmax)\n                probs = torch.softmax(outputs.logits, dim=1)\n                \n                all_preds.append(probs.cpu().numpy())\n                all_labels.append(labels.cpu().numpy())\n                \n        return np.concatenate(all_labels), np.concatenate(all_preds)\n\n    def save(self, path=\"vit_model.pth\"):\n        torch.save(self.model.state_dict(), path)\n        print(f\" Model saved to: {path}\")\n\n    def load(self, path=\"vit_model.pth\"):\n        self.model.load_state_dict(torch.load(path))\n        self.model.eval()\n        print(f\" Model loaded from: {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:48:39.635256Z","iopub.execute_input":"2025-12-12T10:48:39.635585Z","iopub.status.idle":"2025-12-12T10:48:39.660964Z","shell.execute_reply.started":"2025-12-12T10:48:39.635565Z","shell.execute_reply":"2025-12-12T10:48:39.660051Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## 2.5 Loss and Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"## 2.6 Fitting\n","metadata":{}},{"cell_type":"markdown","source":"### 2.6.1 Early Stopping for keras models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',        # Watch validation loss (metric of truth)\n    patience=3,                # Wait 3 epochs before stopping (gives it a chance to recover)\n    restore_best_weights=True, # CRITICAL: Go back to the best weights, not the last ones\n    verbose=1                  # Print a message when it triggers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:45:19.711500Z","iopub.execute_input":"2025-12-12T09:45:19.711786Z","iopub.status.idle":"2025-12-12T09:45:19.718244Z","shell.execute_reply.started":"2025-12-12T09:45:19.711768Z","shell.execute_reply":"2025-12-12T09:45:19.717490Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Function for training, predicting and cleaning (llm) for keras models","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt  # Required to render plots inside a function\n\n# Initialize the global results dictionary if it doesn't exist\nif 'all_results' not in globals():\n    all_results = {}\n\n# Define your test sets map\ntest_sets = {\n    \"CIFAKE_Test\": ds_test_cifake,\n    \"OOD_Test\":    ds_test_ood\n}\n\ndef train_eval_clean(model, model_name, train_ds, val_ds, test_sets, epochs=30):\n    \"\"\"\n    1. Trains the model.\n    2. PLOTS the history (New Step).\n    3. Evaluates on all test/OOD sets.\n    4. Saves results to global 'all_results'.\n    5. Saves model to disk.\n    6. Deletes model and clears VRAM.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\" STARTING PIPELINE FOR: {model_name}\")\n    print(f\"{'='*60}\")\n\n    # ---------------------------------------------------------\n    # 1. TRAIN\n    # ---------------------------------------------------------\n    print(f\" Training {model_name}...\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        verbose=1,\n        callbacks=[early_stopping] # Assumes 'early_stopping' is defined globally\n    )\n\n    # ---------------------------------------------------------\n    # 1.5. PLOT HISTORY (Added)\n    # ---------------------------------------------------------\n    print(f\" Plotting training history for {model_name}...\")\n    \n    # Convert history to pandas DataFrame\n    history_frame = pd.DataFrame(history.history)\n    \n    # Plot Loss\n    # We use plt.show() to force the plot to render immediately during function execution\n    history_frame.loc[:, ['loss', 'val_loss']].plot(title=f\"{model_name} - Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show() \n    \n    # Plot Accuracy (Checks if 'binary_accuracy' exists, falls back to 'accuracy' if needed)\n    acc_key = 'binary_accuracy' if 'binary_accuracy' in history_frame.columns else 'accuracy'\n    val_acc_key = f'val_{acc_key}'\n    \n    if acc_key in history_frame.columns:\n        history_frame.loc[:, [acc_key, val_acc_key]].plot(title=f\"{model_name} - Accuracy\")\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.show()\n\n    # ---------------------------------------------------------\n    # 2. EVALUATE\n    # ---------------------------------------------------------\n    print(f\"\\n Evaluating {model_name} on {len(test_sets)} datasets...\")\n    \n    # Create entry in global results dict\n    all_results[model_name] = {}\n\n    for ds_name, ds in test_sets.items():\n        print(f\"    Predicting on {ds_name}...\")\n        \n        # A. Predict\n        preds = model.predict(ds, verbose=0) # verbose=0 to keep logs clean\n        \n        # Handle shapes: (N, 2) -> (N,) or (N, 1) -> (N,)\n        if preds.shape[-1] > 1:\n            y_pred = preds[:, 1] \n        else:\n            y_pred = preds.flatten()\n            \n        # B. Get True Labels\n        y_true = np.concatenate([y for x, y in ds], axis=0)\n        \n        # C. Store\n        all_results[model_name][ds_name] = {\n            'y_pred': y_pred,\n            'y_true': y_true\n        }\n\n    # ---------------------------------------------------------\n    # 3. SAVE TO DISK\n    # ---------------------------------------------------------\n    filename = f\"{model_name}_final.keras\"\n    model.save(filename)\n    print(f\"\\n Model saved to: {filename}\")\n\n    # ---------------------------------------------------------\n    # 4. CLEANUP (The most important part)\n    # ---------------------------------------------------------\n    print(f\" Scrubbing VRAM...\")\n    del model\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\" {model_name} pipeline complete. GPU is ready for next model.\\n\")\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:45:22.453679Z","iopub.execute_input":"2025-12-12T09:45:22.454319Z","iopub.status.idle":"2025-12-12T09:45:22.464101Z","shell.execute_reply.started":"2025-12-12T09:45:22.454295Z","shell.execute_reply":"2025-12-12T09:45:22.463221Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### 2.6.2 Custom CNN","metadata":{}},{"cell_type":"code","source":"hist_custom = train_eval_clean(\n    custom_cnn, \n    \"Custom_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=2 # Change to 30 for real run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:45:27.532181Z","iopub.execute_input":"2025-12-12T09:45:27.532877Z","iopub.status.idle":"2025-12-12T10:10:23.099641Z","shell.execute_reply.started":"2025-12-12T09:45:27.532852Z","shell.execute_reply":"2025-12-12T10:10:23.098758Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n STARTING PIPELINE FOR: Custom_CNN\n============================================================\n Training Custom_CNN...\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1765532733.069220     101 service.cc:148] XLA service 0x7d08b0006450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1765532733.070232     101 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1765532733.070254     101 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1765532733.766838     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"      1/Unknown \u001b[1m22s\u001b[0m 22s/step - accuracy: 0.5469 - loss: 0.6985","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765532749.225258     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"   3750/Unknown \u001b[1m501s\u001b[0m 128ms/step - accuracy: 0.6761 - loss: 0.5719","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3750/3750\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 152ms/step - accuracy: 0.6761 - loss: 0.5719 - val_accuracy: 0.6166 - val_loss: 0.7044\nEpoch 2/2\n\u001b[1m3750/3750\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 141ms/step - accuracy: 0.7585 - loss: 0.4625 - val_accuracy: 0.6161 - val_loss: 0.7893\nRestoring model weights from the end of the best epoch: 1.\n\n Evaluating Custom_CNN on 2 datasets...\n    Predicting on CIFAKE_Test...\n    Predicting on OOD_Test...\n","output_type":"stream"},{"name":"stderr","text":"Invalid SOS parameters for sequential JPEG\nInvalid SOS parameters for sequential JPEG\n","output_type":"stream"},{"name":"stdout","text":"\n Model saved to: Custom_CNN_final.keras\n Scrubbing VRAM...\n Custom_CNN pipeline complete. GPU is ready for next model.\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### 2.6.3 Pre-trained CNN","metadata":{}},{"cell_type":"code","source":"hist_cnn = train_eval_clean(\n    model_cnn, \n    \"Pretrained_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=2 # Change to 30 for real run\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:10:54.227783Z","iopub.execute_input":"2025-12-12T10:10:54.228115Z","iopub.status.idle":"2025-12-12T10:29:01.757573Z","shell.execute_reply.started":"2025-12-12T10:10:54.228087Z","shell.execute_reply":"2025-12-12T10:29:01.756903Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n STARTING PIPELINE FOR: Pretrained_CNN\n============================================================\n Training Pretrained_CNN...\nEpoch 1/2\n\u001b[1m3750/3750\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 96ms/step - accuracy: 0.7314 - loss: 0.5168 - val_accuracy: 0.7561 - val_loss: 0.4935\nEpoch 2/2\n\u001b[1m3750/3750\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 93ms/step - accuracy: 0.7620 - loss: 0.4738 - val_accuracy: 0.7595 - val_loss: 0.4850\nRestoring model weights from the end of the best epoch: 2.\n\n Evaluating Pretrained_CNN on 2 datasets...\n    Predicting on CIFAKE_Test...\n    Predicting on OOD_Test...\n","output_type":"stream"},{"name":"stderr","text":"Invalid SOS parameters for sequential JPEG\nInvalid SOS parameters for sequential JPEG\n","output_type":"stream"},{"name":"stdout","text":"\n Model saved to: Pretrained_CNN_final.keras\n Scrubbing VRAM...\n Pretrained_CNN pipeline complete. GPU is ready for next model.\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### 2.6.3 Visual Transformer","metadata":{}},{"cell_type":"code","source":"# 1. Initialize\nvit_model = ViTClassifier(num_labels=2)\n\n# 2. Train (Pass val set and patience)\n# Note: Ensure datasets are Unbatched in TF or handle batching inside appropriately.\n# Since my class assumes tfds.as_numpy iterates batches, ensure your input DS is BATCHED.\nvit_model.train(ds_train_full, ds_val_full, epochs=2, patience=3)\n\n# 3. Plot\nvit_model.plot_history()\n\n# Predictions on test sets\n# 1. Initialize the entry for this specific model\nmodel_name = \"ViT_Base_Frozen\"\nall_results[model_name] = {}\n\nprint(f\" Starting Evaluation for {model_name}...\")\n\n# 2. Iterate through your existing test_sets map\nfor ds_name, ds in test_sets.items():\n    print(f\"    Predicting on {ds_name}...\")\n    \n    # Run inference using the class method\n    # This returns (True Labels, Predicted Probabilities [N, 2])\n    y_true, y_probs = vit_model.predict(ds)\n    \n    # We want the probability of the \"Positive\" class (Index 1)\n    # y_probs is shape [N, 2], so we take all rows, column 1\n    y_pred = y_probs[:, 1]\n    \n    # 3. Store in the global dictionary\n    all_results[model_name][ds_name] = {\n        'y_pred': y_pred,\n        'y_true': y_true\n    }\n\nprint(f\"\\n Results for {model_name} saved to 'all_results'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:48:47.005897Z","iopub.execute_input":"2025-12-12T10:48:47.006185Z"}},"outputs":[{"name":"stdout","text":" Initializing ViTClassifier on cuda...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n Starting Training (Max Epochs: 2, Patience: 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0batch [00:00, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1192cab40114385b875b832f9b626cb"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# 3. Results","metadata":{}},{"cell_type":"markdown","source":"### Note for Johannes/Anna.\n\nthe results for all the models can be accessed through the dictionary `all_results`. If you want to load in trained model then use the `tf.keras.models.load_model(\"model_name\")` for the custom CNN and the pre-trained cnn. This way you can make more predictions with the models. ","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Hold-out set performance ","metadata":{}},{"cell_type":"code","source":"# the hold-out set is the ds_test_cifake","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Out-of-distribution set performance","metadata":{}},{"cell_type":"code","source":"# the ood set is the ds_test_ood","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Performance on custom images ","metadata":{}},{"cell_type":"code","source":"# use a pic of johannes and a pic of ai generated johannes to see if it predicts correctly ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Conclusion/Discussion","metadata":{}},{"cell_type":"markdown","source":"possible discussion points:\n> fine tune the base layers in both pre-trained classifier and visual transformer rather than just the classifier head (time/memory constraints).\n\n\n> Modify the custom cnn to see how simple it can be made while still maintaining good performance. ","metadata":{}},{"cell_type":"markdown","source":"# 5. References","metadata":{}},{"cell_type":"markdown","source":"# 6. Division of Labour","metadata":{}}]}