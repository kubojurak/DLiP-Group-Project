{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726,"isSourceIdPinned":false},{"sourceId":9251202,"sourceType":"datasetVersion","datasetId":5596873},{"sourceId":11473304,"sourceType":"datasetVersion","datasetId":7190440},{"sourceId":11973689,"sourceType":"datasetVersion","datasetId":7180950},{"sourceId":685970,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":520311,"modelId":534606},{"sourceId":685973,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":520314,"modelId":534609},{"sourceId":685974,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":520315,"modelId":534610}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#5F9EA0; \n            color:white; \n            padding:15px; \n            border-radius:10px; \n            text-align:center; \n            font-size:30px; \n            font-weight:bold;\">\n    Detecting Deep Fakes - A Deep Learning Computer Vision Task<br>\n    <span style=\"font-size:20px; font-weight:normal;\">\n        Hairy Feet: Anna, Jacob, Johannes\n    </span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n    1. Introduction: Project Overview & Data \n</h1","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nIn this project, we build a deep learning computer vision model to distinguish between real human faces, and AI-generated (deepfake) face images. \n\nWe use the **DeepDetect-2025** dataset from Kaggle, which contains over 100k labeled images of faces, split into two classes: real and fake. The goal is to train a binary classifier that can automatically detect whether an image is genuine or AI generated.","metadata":{}},{"cell_type":"markdown","source":"# 2. Methods","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Importing and Preparing Data","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n\nThe DeepDetect-2025 dataset is organized into separate \"train\" and \"test\" folders, each containing two subfolders:\n\n- \"real\" = real human face images  \n- \"fake\" = AI-generated (deepfake) face images  \n\nWe will:\n- Set a fixed image size and batch size\n- Point TensorFlow to the \"train\" and \"test\" directories\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ncifake_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\ncifake_test_dir  = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test\"\n\ndf20k_dir = \"/kaggle/input/deepfake-vs-real-20k/Deep-vs-Real\"\n\nhf_dir = \"/kaggle/input/human-faces-dataset/Human Faces Dataset\"\n\nds200k_dir = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/my_real_vs_ai_dataset/my_real_vs_ai_dataset\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:32:58.108998Z","iopub.execute_input":"2025-12-12T11:32:58.109508Z","iopub.status.idle":"2025-12-12T11:32:58.113402Z","shell.execute_reply.started":"2025-12-12T11:32:58.109486Z","shell.execute_reply":"2025-12-12T11:32:58.112640Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nNow we create the training, validation and test sets. \nWe use tf.keras.utils.image_dataset_from_directory to:\n\n- Load images from the \"train\"  directory\n- Automatically split the training data into:\n  - **80% training**\n  - **20% validation**\n\nThe images in the \"test\" directory are used as a separate held-out test set that we will only use for final evaluation.","metadata":{}},{"cell_type":"code","source":"# Helper function for getting dataset\ndef create_ds(directory, subset, labels, IMG_SIZE=(224,224), BATCH_SIZE=None, shuffle=True):\n    ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        validation_split=0.2 if subset else None, # Only split if subset is asked for\n        subset=subset,\n        seed=2025,\n        class_names=labels,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        label_mode='int',\n        shuffle=shuffle \n    )\n    return ds\n\n# --- CIFAKE ---\nds_train_cifake = create_ds(cifake_train_dir, 'training', ['FAKE', 'REAL'])\nds_val_cifake   = create_ds(cifake_train_dir, 'validation', ['FAKE', 'REAL'])\n\nds_train_cifake = ds_train_cifake.apply(tf.data.experimental.ignore_errors()) # Because it has corrupted files that crash the system otherwise\n\n#-- 200K Dataset--\nds_train_200k = create_ds(ds200k_dir, 'training', ['ai_images', 'real'])\nds_val_200k   = create_ds(ds200k_dir, 'validation', ['ai_images', 'real'])\n\n# CIFAKE hold-out set\nds_test_cifake = create_ds(cifake_test_dir, None, ['FAKE', 'REAL'], shuffle = False)\n\n# Out-of-Distribution (face) sets\n\n# --- Deepfake 20k ---\nds_ood_df20k = create_ds(df20k_dir, None, ['Deepfake', 'Real'], shuffle = False)\n\n# --- HF Dataset ---\nds_ood_hf = create_ds(hf_dir, None , ['AI-Generated Images', 'Real Images'], shuffle = False)\n\n# 2. Concatenate them (Append HF to the end of DF20k)\nds_ood_combined = ds_ood_df20k.concatenate(ds_ood_hf)\n\nprint(\"Datasets created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:33:04.975263Z","iopub.execute_input":"2025-12-12T11:33:04.975855Z","iopub.status.idle":"2025-12-12T11:44:28.191610Z","shell.execute_reply.started":"2025-12-12T11:33:04.975831Z","shell.execute_reply":"2025-12-12T11:44:28.190984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Define a helper to enforce shapes\ndef force_shape(image, label):\n    # Explicitly set the shape. This converts RaggedTensors to Dense Tensors.\n    image = tf.ensure_shape(image, (224, 224, 3))\n    label = tf.ensure_shape(label, ())\n    return image, label\n\n# Full dataset function\ndef ds_full(datasets, weights, batch_size = 64):\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    ds_full = tf.data.Dataset.sample_from_datasets(\n        datasets,\n        weights=weights, # Optional: Balance the datasets if one is huge\n        stop_on_empty_dataset=False\n    )\n \n    #ds_full = ds_full.unbatch() # Safety check to stop code from crashing if cell is run twice\n    \n    ds_full = ds_full.map(force_shape, num_parallel_calls=tf.data.AUTOTUNE)\n\n    ds_full = ds_full.shuffle(buffer_size=1000)\n    ds_full = ds_full.batch(batch_size)\n    ds_full = ds_full.prefetch(tf.data.AUTOTUNE)\n\n    return ds_full\n\n# Test set function\ndef prepare_test_set(ds, batch_size=64):\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    #ds = ds.unbatch() # Safety check to stop code from crashing if cell is run twice\n    \n    ds = ds.map(force_shape, num_parallel_calls=AUTOTUNE)\n    \n    ds = ds.batch(batch_size)\n    \n    ds = ds.prefetch(AUTOTUNE)\n    \n    # NOTE: No shuffle(), No sample_from_datasets()\n    return ds\n\n# Full training set\nds_train_full = ds_full([ds_train_cifake, ds_train_200k], [1.0, 1.0])\nprint(\"Train Dataset Created!\")\n\n# Full validation set\nds_val_full = ds_full([ds_val_cifake, ds_val_200k], [1.0, 1.0])\nprint(\"Validation Dataset Created!\")\n\n# Test set\nds_test_cifake = prepare_test_set(ds_test_cifake)\n\nds_test_ood = prepare_test_set(ds_ood_combined)\nprint(\"Test Datasets Created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:49:49.135484Z","iopub.execute_input":"2025-12-12T11:49:49.136325Z","iopub.status.idle":"2025-12-12T11:49:49.223988Z","shell.execute_reply.started":"2025-12-12T11:49:49.136300Z","shell.execute_reply":"2025-12-12T11:49:49.223393Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Data Exploration and Visualisation","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Show sample images from the training dataset\nWe visualize sample images by taking the first batch from the training dataset. Since the dataset is shuffled on loading, each batch is a random collection of images. From this batch, we display the first 9 images in a 3Ã—3 grid along with their corresponding class labels (â€œrealâ€ or â€œfakeâ€).\n\nWe do this to perform an initial quality check of the dataset: visual inspection allows us to confirm that the images were loaded correctly, that the labels correspond to the expected classes, and that there are no obvious issues such as corrupted files, incorrect preprocessing, or mislabeled images. Showing random samples also helps us get an intuitive understanding of what the model will see during training and whether the dataset contains sufficient visual variability for effective learning.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.2 Check class distribution\nWe assess the class distribution by counting how many images belong to each category (â€œrealâ€ and â€œfakeâ€). A balanced dataset is important because severe class imbalance can bias the model toward predicting the majority class. By examining the distribution visually and numerically, we ensure that the model will be trained on approximately equal amounts of real and AI-generated images, reducing the risk of skewed learning or misclassification patterns.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.3 Inspect Image Shape and Label Format\nWe inspect the shape of one batch to verify that the images and labels are formatted as expected. The image batch typically has the shape (batch_size, height, width, channels) e.g., (32, 224, 224, 3), indicating 32 RGB images of size 224Ã—224 pixels.\n\nThe label batch should have the shape (batch_size,), containing one integer label per image. Confirming these shapes helps ensure that the data pipeline is correctly configured before building the neural network model.","metadata":{}},{"cell_type":"markdown","source":"The batch shape (32, 224, 224, 3) confirms that images are correctly loaded in batches of 32, each resized to 224Ã—224 pixels with three RGB channels. The label batch (32,) shows that each image has one corresponding class label. The example label 0/1 indicates that the labeling system is functioning as expected and correctly maps images to their respective classes. Overall, the data pipeline is properly structured for model training.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### 2.3.1 Data Augmentation\nWe apply data augmentation to artificially increase the variability of the training dataset and improve the modelâ€™s ability to generalize. The augmentation pipeline randomly flips, rotates, and zooms images during training, introducing meaningful variations that help the model become more robust to common transformations. This reduces overfitting and improves performance on unseen data.\n","metadata":{}},{"cell_type":"markdown","source":"### 2.3.2 Visualizing Data Augmentation\nWe visualize a batch of augmented images to verify that the applied transformations (flipping, rotation, and zoom) behave as intended and produce realistic variations. This quick inspection ensures that augmentation does not distort the data in a harmful way and confirms that the model will receive meaningful, correctly transformed inputs during training.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Neural Models","metadata":{}},{"cell_type":"markdown","source":"### 2.4.1 Custom CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Custom-made CNN classifier \ncustom_cnn = keras.Sequential([\n    # Input layer\n    layers.Input(shape=(224, 224, 3)),\n\n    # Data Augmentation\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    \n    # First convolution + maxpool block (32 filters)\n    layers.Conv2D(filters=32, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Second convolution + maxpool block (64 filters)\n    layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Third convolution + maxpool block (128 filters)\n    layers.Conv2D(filters=128, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    #Fourth convolution + maxpool block (256 filters)\n    layers.Conv2D(filters=256, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),\n    # removed maxpool to preserve fine-grained details\n\n    # Global average pooling block\n    layers.GlobalAveragePooling2D(),\n\n    # Fully connected block\n    layers.Dense(128, activation='relu'), # only one layer with high dropout to fight overfitting\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid'),\n    \n])\n\ncustom_cnn.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # Slower, more precise training\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\ncustom_cnn.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:49:59.312802Z","iopub.execute_input":"2025-12-12T11:49:59.313388Z","iopub.status.idle":"2025-12-12T11:50:00.224381Z","shell.execute_reply.started":"2025-12-12T11:49:59.313366Z","shell.execute_reply":"2025-12-12T11:50:00.223661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4.2 Pre-trained CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**\n\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nIMG_SIZE = 224\n\n# 1. The Augmentation Block (CRITICAL: Put this back!)\n# We place this inside the model so it runs on GPU during training\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n])\n\n# 2. Load Pretrained EfficientNetB0\nbase_model = tf.keras.applications.EfficientNetB0(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\nbase_model.trainable = False  # Freeze weights\n\n# 3. Build the final Model\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n\n# augmentation applies ONLY during training\nx = data_augmentation(inputs) \n\n# EfficientNet expects [0-255] values. \n# We explicitly do NOT use Rescaling(1./255) here.\n# 'preprocess_input' is technically a pass-through for EfficientNet, but good practice to keep.\nx = tf.keras.applications.efficientnet.preprocess_input(x)\n\n# Run through the frozen base\nx = base_model(x, training=False) # Keep BatchNormalization layers in inference mode\n\n# Classifier Head\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel_cnn = models.Model(inputs, outputs)\n\nmodel_cnn.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n    loss='binary_crossentropy', \n    metrics=['accuracy']\n)\n\nmodel_cnn.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:50:06.317819Z","iopub.execute_input":"2025-12-12T11:50:06.318516Z","iopub.status.idle":"2025-12-12T11:50:07.770451Z","shell.execute_reply.started":"2025-12-12T11:50:06.318491Z","shell.execute_reply":"2025-12-12T11:50:07.769847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4.3 Visual Transformer","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# 1ï¸âƒ£ Imports & Setup\n# ------------------------------\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\n# CHANGED: Import Swin Transformer\nfrom transformers import SwinForImageClassification \nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport copy\nimport os\n\n# Enable faster TF GPU memory allocation\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nclass SwinClassifier:\n    def __init__(self, model_name=\"microsoft/swin-tiny-patch4-window7-224\", num_labels=1, learning_rate=2e-5):\n        \"\"\"\n        Initializes the Swin Transformer.\n        Defaults to Swin-Tiny (28M params) for efficiency.\n        \"\"\"\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"ðŸ”§ Initializing SwinClassifier on {self.device}...\")\n\n        # 1. Load Model\n        self.model = SwinForImageClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels,\n            ignore_mismatched_sizes=True\n        ).to(self.device)\n\n        # ---------------------------------------------------------\n        # â„ï¸ FREEZING (Critical for Swin)\n        # ---------------------------------------------------------\n        # For Swin, the backbone is stored in .swin (not .vit)\n        for param in self.model.swin.parameters():\n            param.requires_grad = False\n            \n        # Ensure the Classifier Head is OPEN for training\n        for param in self.model.classifier.parameters():\n            param.requires_grad = True\n        # ---------------------------------------------------------\n\n        # 2. Compile (Optional Speedup for P100/T4 GPUs)\n        try:\n            self.model = torch.compile(self.model)\n        except:\n            pass\n\n        # 3. Optimization\n        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n        self.optimizer = AdamW(trainable_params, lr=learning_rate)\n        \n        # CHANGED: Use BCEWithLogitsLoss for Binary Classification (1 neuron)\n        # This is more numerically stable than CrossEntropy for 2 classes.\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.scaler = torch.amp.GradScaler('cuda')\n\n        self.history = {\n            'loss': [], 'accuracy': [],\n            'val_loss': [], 'val_accuracy': []\n        }\n\n    def _tf_to_torch_fast(self, images, labels):\n        \"\"\"\n        Internal helper: Optimized data transfer from TF (CPU) -> PyTorch (GPU).\n        \"\"\"\n        images = torch.from_numpy(np.array(images)) \n        \n        # CHANGED: Convert labels to float and add dimension for BCE Loss\n        # Input: [0, 1, 0] -> Output: [[0.0], [1.0], [0.0]]\n        labels = torch.from_numpy(np.array(labels)).float().to(self.device)\n        labels = labels.unsqueeze(1) \n\n        images = images.to(self.device, non_blocking=True)\n        # Swin expects [Batch, Channel, Height, Width]\n        images = images.permute(0, 3, 1, 2).float() / 255.0\n        \n        return images, labels\n\n    def _validate(self, val_loader):\n        \"\"\"Internal helper for validation loop\"\"\"\n        self.model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = self._tf_to_torch_fast(images, labels)\n                \n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(images)\n                    loss = self.criterion(outputs.logits, labels)\n                \n                val_loss += loss.item() * labels.size(0)\n                \n                # CHANGED: Binary Accuracy (Sigmoid > 0.5)\n                probs = torch.sigmoid(outputs.logits)\n                preds = (probs > 0.5).float()\n                \n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        \n        if total == 0: return 0.0, 0.0\n        return val_loss / total, correct / total\n\n    def train(self, train_ds, val_ds, epochs=10, patience=3):\n        \"\"\"\n        Main training loop with Swin-specific adjustments.\n        \"\"\"\n        try:\n            n_batches = len(train_ds)\n        except:\n            n_batches = None\n\n        train_loader = tfds.as_numpy(train_ds)\n        val_loader = tfds.as_numpy(val_ds)\n        \n        best_val_loss = float('inf')\n        patience_counter = 0\n        best_model_weights = None\n        \n        print(f\"\\nðŸš€ Starting Swin Training (Max Epochs: {epochs}, Patience: {patience})\")\n\n        for epoch in range(epochs):\n            self.model.train()\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            with tqdm(train_loader, total=n_batches, unit=\"batch\", leave=True) as pbar:\n                pbar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n                \n                for batch_idx, (images, labels) in enumerate(pbar):\n                    images, labels = self._tf_to_torch_fast(images, labels)\n                    \n                    self.optimizer.zero_grad()\n                    \n                    # Mixed Precision Forward\n                    with torch.amp.autocast('cuda'):\n                        outputs = self.model(images)\n                        loss = self.criterion(outputs.logits, labels)\n                    \n                    # Mixed Precision Backward\n                    self.scaler.scale(loss).backward()\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n\n                    # Metrics\n                    batch_size = labels.size(0)\n                    running_loss += loss.item() * batch_size\n                    \n                    # CHANGED: Binary Accuracy\n                    probs = torch.sigmoid(outputs.logits)\n                    preds = (probs > 0.5).float()\n                    \n                    correct += (preds == labels).sum().item()\n                    total += batch_size\n                    \n                    # Live Update\n                    current_acc = correct / total\n                    current_loss = running_loss / total\n                    pbar.set_postfix({\"loss\": f\"{current_loss:.4f}\", \"acc\": f\"{current_acc:.4f}\"})\n\n            # Validation\n            val_loss, val_acc = self._validate(val_loader)\n            \n            self.history['loss'].append(current_loss)\n            self.history['accuracy'].append(current_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_accuracy'].append(val_acc)\n            \n            print(f\"    Validation - loss: {val_loss:.4f} - acc: {val_acc:.4f}\")\n\n            # Early Stopping\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                best_model_weights = copy.deepcopy(self.model.state_dict())\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"\\nðŸ›‘ Early stopping triggered! Restoring best weights.\")\n                    self.model.load_state_dict(best_model_weights)\n                    break\n        \n        if best_model_weights is not None and patience_counter < patience:\n             self.model.load_state_dict(best_model_weights)\n\n    def plot_history(self):\n        \"\"\"Plots accuracy and loss graphs.\"\"\"\n        if not self.history['loss']:\n            print(\"No training history to plot.\")\n            return\n\n        epochs_range = range(1, len(self.history['loss']) + 1)\n        \n        plt.figure(figsize=(12, 5))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs_range, self.history['loss'], 'bo-', label='Training Loss')\n        plt.plot(epochs_range, self.history['val_loss'], 'r-', label='Validation Loss')\n        plt.title('Loss')\n        plt.legend()\n        plt.grid(True)\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs_range, self.history['accuracy'], 'bo-', label='Training Acc')\n        plt.plot(epochs_range, self.history['val_accuracy'], 'r-', label='Validation Acc')\n        plt.title('Accuracy')\n        plt.legend()\n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def predict(self, tf_dataset):\n        \"\"\"Returns predictions for the dataset.\"\"\"\n        loader = tfds.as_numpy(tf_dataset)\n        self.model.eval()\n        \n        all_preds = []\n        all_labels = []\n        \n        print(f\"ðŸ” Running Prediction...\")\n        with torch.no_grad():\n            for images, labels in tqdm(loader, unit=\"batch\"):\n                images, labels = self._tf_to_torch_fast(images, labels)\n                \n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(images)\n                \n                # CHANGED: Sigmoid for Binary Probabilities\n                probs = torch.sigmoid(outputs.logits)\n                \n                all_preds.append(probs.cpu().numpy())\n                all_labels.append(labels.cpu().numpy())\n                \n        return np.concatenate(all_labels), np.concatenate(all_preds)\n\n    def save(self, path=\"swin_model.pth\"):\n        torch.save(self.model.state_dict(), path)\n        print(f\"ðŸ’¾ Model saved to: {path}\")\n\n    def load(self, path=\"swin_model.pth\"):\n        self.model.load_state_dict(torch.load(path))\n        self.model.eval()\n        print(f\"ðŸ“‚ Model loaded from: {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:50:14.070679Z","iopub.execute_input":"2025-12-12T11:50:14.071175Z","iopub.status.idle":"2025-12-12T11:50:32.790280Z","shell.execute_reply.started":"2025-12-12T11:50:14.071152Z","shell.execute_reply":"2025-12-12T11:50:32.789623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.5 Loss and Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"## 2.6 Fitting\n","metadata":{}},{"cell_type":"markdown","source":"### 2.6.1 Early Stopping for keras models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',        # Watch validation loss (metric of truth)\n    patience=3,                # Wait 3 epochs before stopping (gives it a chance to recover)\n    restore_best_weights=True, # CRITICAL: Go back to the best weights, not the last ones\n    verbose=1                  # Print a message when it triggers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:51:02.935687Z","iopub.execute_input":"2025-12-12T11:51:02.936013Z","iopub.status.idle":"2025-12-12T11:51:02.940106Z","shell.execute_reply.started":"2025-12-12T11:51:02.935976Z","shell.execute_reply":"2025-12-12T11:51:02.939381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Function for training, predicting and cleaning (llm) for keras models","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt  # Required to render plots inside a function\n\n# Initialize the global results dictionary if it doesn't exist\nif 'all_results' not in globals():\n    all_results = {}\n\n# Define your test sets map\ntest_sets = {\n    \"CIFAKE_Test\": ds_test_cifake,\n    \"OOD_Test\":    ds_test_ood\n}\n\ndef train_eval_clean(model, model_name, train_ds, val_ds, test_sets, epochs=30):\n    \"\"\"\n    1. Trains the model.\n    2. PLOTS the history (New Step).\n    3. Evaluates on all test/OOD sets.\n    4. Saves results to global 'all_results'.\n    5. Saves model to disk.\n    6. Deletes model and clears VRAM.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ STARTING PIPELINE FOR: {model_name}\")\n    print(f\"{'='*60}\")\n\n    # ---------------------------------------------------------\n    # 1. TRAIN\n    # ---------------------------------------------------------\n    print(f\"ðŸ“‰ Training {model_name}...\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        verbose=1,\n        callbacks=[early_stopping] # Assumes 'early_stopping' is defined globally\n    )\n\n    # ---------------------------------------------------------\n    # 1.5. PLOT HISTORY (Added)\n    # ---------------------------------------------------------\n    print(f\"ðŸ“Š Plotting training history for {model_name}...\")\n    \n    # Convert history to pandas DataFrame\n    history_frame = pd.DataFrame(history.history)\n    \n    # Plot Loss\n    # We use plt.show() to force the plot to render immediately during function execution\n    history_frame.loc[:, ['loss', 'val_loss']].plot(title=f\"{model_name} - Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show() \n    \n    # Plot Accuracy (Checks if 'binary_accuracy' exists, falls back to 'accuracy' if needed)\n    acc_key = 'binary_accuracy' if 'binary_accuracy' in history_frame.columns else 'accuracy'\n    val_acc_key = f'val_{acc_key}'\n    \n    if acc_key in history_frame.columns:\n        history_frame.loc[:, [acc_key, val_acc_key]].plot(title=f\"{model_name} - Accuracy\")\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.show()\n\n    # ---------------------------------------------------------\n    # 2. EVALUATE\n    # ---------------------------------------------------------\n    print(f\"\\nðŸ” Evaluating {model_name} on {len(test_sets)} datasets...\")\n    \n    # Create entry in global results dict\n    all_results[model_name] = {}\n\n    for ds_name, ds in test_sets.items():\n        print(f\"   â€¢ Predicting on {ds_name}...\")\n        \n        # A. Predict\n        preds = model.predict(ds, verbose=0) # verbose=0 to keep logs clean\n        \n        # Handle shapes: (N, 2) -> (N,) or (N, 1) -> (N,)\n        if preds.shape[-1] > 1:\n            y_pred = preds[:, 1] \n        else:\n            y_pred = preds.flatten()\n            \n        # B. Get True Labels\n        y_true = np.concatenate([y for x, y in ds], axis=0)\n        \n        # C. Store\n        all_results[model_name][ds_name] = {\n            'y_pred': y_pred,\n            'y_true': y_true\n        }\n\n    # ---------------------------------------------------------\n    # 3. SAVE TO DISK\n    # ---------------------------------------------------------\n    filename = f\"{model_name}_final.keras\"\n    model.save(filename)\n    print(f\"\\nðŸ’¾ Model saved to: {filename}\")\n\n    # ---------------------------------------------------------\n    # 4. CLEANUP (The most important part)\n    # ---------------------------------------------------------\n    print(f\"ðŸ§¹ Scrubbing VRAM...\")\n    del model\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\"âœ¨ {model_name} pipeline complete. GPU is ready for next model.\\n\")\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:51:05.675773Z","iopub.execute_input":"2025-12-12T11:51:05.676537Z","iopub.status.idle":"2025-12-12T11:51:05.686001Z","shell.execute_reply.started":"2025-12-12T11:51:05.676510Z","shell.execute_reply":"2025-12-12T11:51:05.685344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6.2 Custom CNN","metadata":{}},{"cell_type":"code","source":"hist_custom = train_eval_clean(\n    custom_cnn, \n    \"Custom_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=20 # Change to 30 for real run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:51:20.854332Z","iopub.execute_input":"2025-12-12T11:51:20.854992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6.3 Pre-trained CNN","metadata":{}},{"cell_type":"code","source":"hist_cnn = train_eval_clean(\n    model_cnn, \n    \"Pretrained_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=10 # Change to 30 for real run\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:10:54.227783Z","iopub.execute_input":"2025-12-12T10:10:54.228115Z","iopub.status.idle":"2025-12-12T10:29:01.757573Z","shell.execute_reply.started":"2025-12-12T10:10:54.228087Z","shell.execute_reply":"2025-12-12T10:29:01.756903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6.3 Visual Transformer","metadata":{}},{"cell_type":"code","source":"# 1. Initialize (Switched to Swin, 1 label for binary)\nswin_model = SwinClassifier(num_labels=1) \n\n# 2. Train \n# Note: Ensure ds_train_full and ds_val_full are BATCHED (e.g., .batch(32))\nswin_model.train(ds_train_full, ds_val_full, epochs=10, patience=3)\n\n# 3. Plot\nswin_model.plot_history()\n\n# Predictions on test sets\n# 1. Initialize the entry for this specific model\nmodel_name = \"Swin_Tiny_Frozen\" # Updated name\nall_results[model_name] = {}\n\nprint(f\"ðŸš€ Starting Evaluation for {model_name}...\")\n\n# 2. Iterate through your existing test_sets map\nfor ds_name, ds in test_sets.items():\n    print(f\"   â€¢ Predicting on {ds_name}...\")\n    \n    # Run inference\n    # Returns:\n    # y_true: [N] (0 or 1)\n    # y_probs: [N, 1] (Probability of class 1, 0.0 to 1.0)\n    y_true, y_probs = swin_model.predict(ds)\n    \n    # CHANGED: No need to slice [:, 1] anymore. \n    # The model output is already the probability of the positive class.\n    # We flatten it from [N, 1] to [N] just to be clean.\n    y_pred = y_probs.flatten()\n   \n    # 3. Store in the global dictionary\n    all_results[model_name][ds_name] = {\n        'y_pred': y_pred,\n        'y_true': y_true\n    }\n\nprint(f\"\\nâœ… Results for {model_name} saved to 'all_results'.\")\n\n# Saving the model\nswin_model.save(\"Swin_Tiny_Frozen_Final.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Results","metadata":{}},{"cell_type":"markdown","source":"### Note for Johannes/Anna.\n\nthe results for all the models can be accessed through the dictionary `all_results`. If you want to load in trained model then use the `tf.keras.models.load_model(\"model_name\")` for the custom CNN and the pre-trained cnn. This way you can make more predictions with the models. ","metadata":{}},{"cell_type":"code","source":"#The paths to the different models \n\ncustom_cnn_path = \"/kaggle/input/custom-cnn/tensorflow2/default/1/Custom_CNN_final.keras\"\n\npretrained_cnn_path = \"/kaggle/input/pre-trained-cnn/tensorflow2/default/1/Pretrained_CNN_final.keras\"\n\nswin_model_path = \"/kaggle/input/swin-model/pytorch/default/1/Swin_Tiny_Frozen_Final.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:06:24.222793Z","iopub.execute_input":"2025-12-18T19:06:24.223468Z","iopub.status.idle":"2025-12-18T19:06:24.226955Z","shell.execute_reply.started":"2025-12-18T19:06:24.223447Z","shell.execute_reply":"2025-12-18T19:06:24.226275Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 3.1 Hold-out set performance ","metadata":{}},{"cell_type":"code","source":"# the hold-out set is the ds_test_cifake\nimport tensorflow as tf\ncustom_cnn_model = tf.keras.models.load_model(custom_cnn_path)\npre_trained_model = tf.keras.models.load_model(pretrained_cnn_path)\n\ncustom_cnn_model.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T19:07:04.725788Z","iopub.execute_input":"2025-12-18T19:07:04.726512Z","iopub.status.idle":"2025-12-18T19:07:06.779636Z","shell.execute_reply.started":"2025-12-18T19:07:04.726487Z","shell.execute_reply":"2025-12-18T19:07:06.779053Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3.2 Out-of-distribution set performance","metadata":{}},{"cell_type":"code","source":"# the ood set is the ds_test_ood","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Performance on custom images ","metadata":{}},{"cell_type":"code","source":"# use a pic of johannes and a pic of ai generated johannes to see if it predicts correctly ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Conclusion/Discussion","metadata":{}},{"cell_type":"markdown","source":"possible discussion points:\n> fine tune the base layers in both pre-trained classifier and visual transformer rather than just the classifier head (time/memory constraints).\n\n\n> Modify the custom cnn to see how simple it can be made while still maintaining good performance. ","metadata":{}},{"cell_type":"markdown","source":"# 5. References","metadata":{}},{"cell_type":"markdown","source":"# 6. Division of Labour","metadata":{}}]}