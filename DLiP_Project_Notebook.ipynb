{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726,"isSourceIdPinned":false},{"sourceId":9251202,"sourceType":"datasetVersion","datasetId":5596873},{"sourceId":11473304,"sourceType":"datasetVersion","datasetId":7190440},{"sourceId":11973689,"sourceType":"datasetVersion","datasetId":7180950}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#5F9EA0; \n            color:white; \n            padding:15px; \n            border-radius:10px; \n            text-align:center; \n            font-size:30px; \n            font-weight:bold;\">\n    Detecting Deep Fakes - A Deep Learning Computer Vision Task<br>\n    <span style=\"font-size:20px; font-weight:normal;\">\n        Hairy Feet: Anna, Jacob, Johannes\n    </span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = setup style = 'font-size:30px; background: linear-gradient(90deg, #5F9EA0, #7AC5CD, #8EE5EE); color: white; padding: 10px;  border-radius: 10px;'>\n    1. Introduction: Project Overview & Data \n</h1","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nIn this project, we build a deep learning computer vision model to distinguish between real human faces, and AI-generated (deepfake) face images. \n\nWe use the **DeepDetect-2025** dataset from Kaggle, which contains over 100k labeled images of faces, split into two classes: real and fake. The goal is to train a binary classifier that can automatically detect whether an image is genuine or AI generated.","metadata":{}},{"cell_type":"markdown","source":"# 2. Methods","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Importing and Preparing Data","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download CIFAKE dataset (for later use)\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download Deepfake 20K\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n\n# Download Human-faces dataset\npath = kagglehub.dataset_download(\"/kaggle/input/human-faces-dataset\")\n\n# Download 200K dataset\npath = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n\nprint(\"Path to dataset files:\", path)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:49:21.822971Z","iopub.execute_input":"2025-12-11T09:49:21.823259Z","iopub.status.idle":"2025-12-11T09:49:22.808956Z","shell.execute_reply.started":"2025-12-11T09:49:21.823237Z","shell.execute_reply":"2025-12-11T09:49:22.808399Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/deepdetect-2025\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\nPath to dataset files: /kaggle/input/cifake-real-and-ai-generated-synthetic-images\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n\nThe DeepDetect-2025 dataset is organized into separate \"train\" and \"test\" folders, each containing two subfolders:\n\n- \"real\" = real human face images  \n- \"fake\" = AI-generated (deepfake) face images  \n\nWe will:\n- Set a fixed image size and batch size\n- Point TensorFlow to the \"train\" and \"test\" directories\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ncifake_train_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train\"\ncifake_test_dir  = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test\"\n\ndf20k_dir = \"/kaggle/input/deepfake-vs-real-20k/Deep-vs-Real\"\n\nhf_dir = \"/kaggle/input/human-faces-dataset/Human Faces Dataset\"\n\nds200k_dir = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/my_real_vs_ai_dataset/my_real_vs_ai_dataset\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:32:58.108998Z","iopub.execute_input":"2025-12-12T11:32:58.109508Z","iopub.status.idle":"2025-12-12T11:32:58.113402Z","shell.execute_reply.started":"2025-12-12T11:32:58.109486Z","shell.execute_reply":"2025-12-12T11:32:58.112640Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"\n<div style=\"background-color:#D2EFF1;\n            color:#333333;\n            padding:12px 16px;\n            border-radius:10px;\n            margin:10px 0;\">\n   </b>\n    \nNow we create the training, validation and test sets. \nWe use tf.keras.utils.image_dataset_from_directory to:\n\n- Load images from the \"train\"  directory\n- Automatically split the training data into:\n  - **80% training**\n  - **20% validation**\n\nThe images in the \"test\" directory are used as a separate held-out test set that we will only use for final evaluation.","metadata":{}},{"cell_type":"code","source":"# Helper function for getting dataset\ndef create_ds(directory, subset, labels, IMG_SIZE=(224,224), BATCH_SIZE=None, shuffle=True):\n    ds = tf.keras.utils.image_dataset_from_directory(\n        directory,\n        validation_split=0.2 if subset else None, # Only split if subset is asked for\n        subset=subset,\n        seed=2025,\n        class_names=labels,\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        label_mode='int',\n        shuffle=shuffle \n    )\n    return ds\n\n# --- CIFAKE ---\nds_train_cifake = create_ds(cifake_train_dir, 'training', ['FAKE', 'REAL'])\nds_val_cifake   = create_ds(cifake_train_dir, 'validation', ['FAKE', 'REAL'])\n\nds_train_cifake = ds_train_cifake.apply(tf.data.experimental.ignore_errors()) # Because it has corrupted files that crash the system otherwise\n\n#-- 200K Dataset--\nds_train_200k = create_ds(ds200k_dir, 'training', ['ai_images', 'real'])\nds_val_200k   = create_ds(ds200k_dir, 'validation', ['ai_images', 'real'])\n\n# CIFAKE hold-out set\nds_test_cifake = create_ds(cifake_test_dir, None, ['FAKE', 'REAL'], shuffle = False)\n\n# Out-of-Distribution (face) sets\n\n# --- Deepfake 20k ---\nds_ood_df20k = create_ds(df20k_dir, None, ['Deepfake', 'Real'], shuffle = False)\n\n# --- HF Dataset ---\nds_ood_hf = create_ds(hf_dir, None , ['AI-Generated Images', 'Real Images'], shuffle = False)\n\n# 2. Concatenate them (Append HF to the end of DF20k)\nds_ood_combined = ds_ood_df20k.concatenate(ds_ood_hf)\n\nprint(\"Datasets created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:33:04.975263Z","iopub.execute_input":"2025-12-12T11:33:04.975855Z","iopub.status.idle":"2025-12-12T11:44:28.191610Z","shell.execute_reply.started":"2025-12-12T11:33:04.975831Z","shell.execute_reply":"2025-12-12T11:44:28.190984Z"}},"outputs":[{"name":"stdout","text":"Found 100000 files belonging to 2 classes.\nUsing 80000 files for training.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765539342.570751      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1765539342.571496      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Found 100000 files belonging to 2 classes.\nUsing 20000 files for validation.\nWARNING:tensorflow:From /tmp/ipykernel_47/1887301689.py:20: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.ignore_errors` instead.\nFound 200000 files belonging to 2 classes.\nUsing 160000 files for training.\nFound 200000 files belonging to 2 classes.\nUsing 40000 files for validation.\nFound 20000 files belonging to 2 classes.\nFound 19219 files belonging to 2 classes.\nFound 9630 files belonging to 2 classes.\nDatasets created!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Define a helper to enforce shapes\ndef force_shape(image, label):\n    # Explicitly set the shape. This converts RaggedTensors to Dense Tensors.\n    image = tf.ensure_shape(image, (224, 224, 3))\n    label = tf.ensure_shape(label, ())\n    return image, label\n\n# Full dataset function\ndef ds_full(datasets, weights, batch_size = 64):\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    ds_full = tf.data.Dataset.sample_from_datasets(\n        datasets,\n        weights=weights, # Optional: Balance the datasets if one is huge\n        stop_on_empty_dataset=False\n    )\n \n    #ds_full = ds_full.unbatch() # Safety check to stop code from crashing if cell is run twice\n    \n    ds_full = ds_full.map(force_shape, num_parallel_calls=tf.data.AUTOTUNE)\n\n    ds_full = ds_full.shuffle(buffer_size=1000)\n    ds_full = ds_full.batch(batch_size)\n    ds_full = ds_full.prefetch(tf.data.AUTOTUNE)\n\n    return ds_full\n\n# Test set function\ndef prepare_test_set(ds, batch_size=64):\n    AUTOTUNE = tf.data.AUTOTUNE\n\n    #ds = ds.unbatch() # Safety check to stop code from crashing if cell is run twice\n    \n    ds = ds.map(force_shape, num_parallel_calls=AUTOTUNE)\n    \n    ds = ds.batch(batch_size)\n    \n    ds = ds.prefetch(AUTOTUNE)\n    \n    # NOTE: No shuffle(), No sample_from_datasets()\n    return ds\n\n# Full training set\nds_train_full = ds_full([ds_train_cifake, ds_train_200k], [1.0, 1.0])\nprint(\"Train Dataset Created!\")\n\n# Full validation set\nds_val_full = ds_full([ds_val_cifake, ds_val_200k], [1.0, 1.0])\nprint(\"Validation Dataset Created!\")\n\n# Test set\nds_test_cifake = prepare_test_set(ds_test_cifake)\n\nds_test_ood = prepare_test_set(ds_ood_combined)\nprint(\"Test Datasets Created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:49:49.135484Z","iopub.execute_input":"2025-12-12T11:49:49.136325Z","iopub.status.idle":"2025-12-12T11:49:49.223988Z","shell.execute_reply.started":"2025-12-12T11:49:49.136300Z","shell.execute_reply":"2025-12-12T11:49:49.223393Z"}},"outputs":[{"name":"stdout","text":"Train Dataset Created!\nValidation Dataset Created!\nTest Datasets Created!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2.2 Data Exploration and Visualisation","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Show sample images from the training dataset\nWe visualize sample images by taking the first batch from the training dataset. Since the dataset is shuffled on loading, each batch is a random collection of images. From this batch, we display the first 9 images in a 3×3 grid along with their corresponding class labels (“real” or “fake”).\n\nWe do this to perform an initial quality check of the dataset: visual inspection allows us to confirm that the images were loaded correctly, that the labels correspond to the expected classes, and that there are no obvious issues such as corrupted files, incorrect preprocessing, or mislabeled images. Showing random samples also helps us get an intuitive understanding of what the model will see during training and whether the dataset contains sufficient visual variability for effective learning.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.2 Check class distribution\nWe assess the class distribution by counting how many images belong to each category (“real” and “fake”). A balanced dataset is important because severe class imbalance can bias the model toward predicting the majority class. By examining the distribution visually and numerically, we ensure that the model will be trained on approximately equal amounts of real and AI-generated images, reducing the risk of skewed learning or misclassification patterns.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.3 Inspect Image Shape and Label Format\nWe inspect the shape of one batch to verify that the images and labels are formatted as expected. The image batch typically has the shape (batch_size, height, width, channels) e.g., (32, 224, 224, 3), indicating 32 RGB images of size 224×224 pixels.\n\nThe label batch should have the shape (batch_size,), containing one integer label per image. Confirming these shapes helps ensure that the data pipeline is correctly configured before building the neural network model.","metadata":{}},{"cell_type":"markdown","source":"The batch shape (32, 224, 224, 3) confirms that images are correctly loaded in batches of 32, each resized to 224×224 pixels with three RGB channels. The label batch (32,) shows that each image has one corresponding class label. The example label 0/1 indicates that the labeling system is functioning as expected and correctly maps images to their respective classes. Overall, the data pipeline is properly structured for model training.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### 2.3.1 Data Augmentation\nWe apply data augmentation to artificially increase the variability of the training dataset and improve the model’s ability to generalize. The augmentation pipeline randomly flips, rotates, and zooms images during training, introducing meaningful variations that help the model become more robust to common transformations. This reduces overfitting and improves performance on unseen data.\n","metadata":{}},{"cell_type":"markdown","source":"### 2.3.2 Visualizing Data Augmentation\nWe visualize a batch of augmented images to verify that the applied transformations (flipping, rotation, and zoom) behave as intended and produce realistic variations. This quick inspection ensures that augmentation does not distort the data in a harmful way and confirms that the model will receive meaningful, correctly transformed inputs during training.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Neural Models","metadata":{}},{"cell_type":"markdown","source":"### 2.4.1 Custom CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Custom-made CNN classifier \ncustom_cnn = keras.Sequential([\n    # Input layer\n    layers.Input(shape=(224, 224, 3)),\n    \n    # First convolution + maxpool block (32 filters)\n    layers.Conv2D(filters=32, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Second convolution + maxpool block (64 filters)\n    layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    # Third convolution + maxpool block (128 filters)\n    layers.Conv2D(filters=128, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),   \n    layers.MaxPool2D(),\n\n    #Fourth convolution + maxpool block (256 filters)\n    layers.Conv2D(filters=256, kernel_size=3, padding='same'),\n    layers.BatchNormalization(), \n    layers.Activation('relu'),\n    # removed maxpool to preserve fine-grained details\n\n    # Global average pooling block\n    layers.GlobalAveragePooling2D(),\n\n    # Fully connected block\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(64,activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid'),\n    \n])\n\ncustom_cnn.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # Slower, more precise training\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\ncustom_cnn.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:49:59.312802Z","iopub.execute_input":"2025-12-12T11:49:59.313388Z","iopub.status.idle":"2025-12-12T11:50:00.224381Z","shell.execute_reply.started":"2025-12-12T11:49:59.313366Z","shell.execute_reply":"2025-12-12T11:50:00.223661Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m433,601\u001b[0m (1.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">433,601</span> (1.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m432,641\u001b[0m (1.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">432,641</span> (1.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n</pre>\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### 2.4.2 Pre-trained CNN","metadata":{}},{"cell_type":"markdown","source":"**Architecture**\n\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nIMG_SIZE = 224\nBATCH_SIZE = 32\n\n# Load pretrained EfficientNetB0\nbase_model = tf.keras.applications.EfficientNetB0(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\nbase_model.trainable = False  # freeze weights for transfer learning\n\n# Build model\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = tf.keras.applications.efficientnet.preprocess_input(inputs)\nx = base_model(x, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel_cnn = models.Model(inputs, outputs)\n\nmodel_cnn.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_cnn.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:50:06.317819Z","iopub.execute_input":"2025-12-12T11:50:06.318516Z","iopub.status.idle":"2025-12-12T11:50:07.770451Z","shell.execute_reply.started":"2025-12-12T11:50:06.318491Z","shell.execute_reply":"2025-12-12T11:50:07.769847Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### 2.4.3 Visual Transformer","metadata":{}},{"cell_type":"markdown","source":"**Architecture**","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# 1️⃣ Imports & Setup\n# ------------------------------\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom transformers import ViTForImageClassification\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm  # Keras-style progress bar\nimport copy\nimport os\n\n# Enable faster TF GPU memory allocation\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nclass ViTClassifier:\n    def __init__(self, model_name=\"google/vit-base-patch16-224\", num_labels=2, learning_rate=2e-5):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"🔧 Initializing ViTClassifier on {self.device}...\")\n\n        # 1. Load Model\n        self.model = ViTForImageClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels,\n            ignore_mismatched_sizes=True\n        ).to(self.device)\n\n        # ---------------------------------------------------------\n        # ❄️ FREEZING \n        # ---------------------------------------------------------\n        # Loop through the base model (self.model.vit) and turn off gradients\n        for param in self.model.vit.parameters():\n            param.requires_grad = False\n            \n        # Ensure the Classifier Head (self.model.classifier) is OPEN for training\n        for param in self.model.classifier.parameters():\n            param.requires_grad = True\n        # ---------------------------------------------------------\n\n        # 2. Compile (Optional Speedup)\n        try:\n            self.model = torch.compile(self.model)\n        except:\n            pass\n\n        # 3. Optimization (Only optimize parameters that require gradients)\n        # We filter the parameters to ensure the optimizer doesn't track frozen ones\n        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n        self.optimizer = AdamW(trainable_params, lr=learning_rate)\n        \n        self.criterion = nn.CrossEntropyLoss()\n        self.scaler = torch.amp.GradScaler('cuda')\n\n        self.history = {\n            'loss': [], 'accuracy': [],\n            'val_loss': [], 'val_accuracy': []\n        }\n\n    def _tf_to_torch_fast(self, images, labels):\n        \"\"\"\n        Internal helper: Optimized data transfer from TF (CPU) -> PyTorch (GPU).\n        Expects images to be uint8 [0, 255].\n        \"\"\"\n        # Zero-copy transfer to CPU tensor\n        images = torch.from_numpy(np.array(images)) \n        labels = torch.from_numpy(np.array(labels)).long().to(self.device)\n\n        # Move to GPU (non_blocking allows async transfer)\n        images = images.to(self.device, non_blocking=True)\n\n        # Permute (Batch, Height, Width, Channel) -> (Batch, Channel, Height, Width)\n        # And Normalize [0, 255] -> [0.0, 1.0]\n        images = images.permute(0, 3, 1, 2).float() / 255.0\n        \n        return images, labels\n\n    def _validate(self, val_loader):\n        \"\"\"Internal helper for validation loop\"\"\"\n        self.model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = self._tf_to_torch_fast(images, labels)\n                \n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(images)\n                    loss = self.criterion(outputs.logits, labels)\n                \n                val_loss += loss.item() * labels.size(0)\n                preds = torch.argmax(outputs.logits, dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        \n        # Avoid division by zero\n        if total == 0: return 0.0, 0.0\n        return val_loss / total, correct / total\n\n    def train(self, train_ds, val_ds, epochs=10, patience=3):\n        \"\"\"\n        Main training loop with:\n         - TQDM Progress Bar (Keras style)\n         - Early Stopping\n         - History tracking\n        \"\"\"\n        # Convert TF datasets to numpy iterables\n        # We try to get the length for the progress bar if possible\n        try:\n            n_batches = len(train_ds)\n        except:\n            n_batches = None\n\n        train_loader = tfds.as_numpy(train_ds)\n        val_loader = tfds.as_numpy(val_ds)\n        \n        best_val_loss = float('inf')\n        patience_counter = 0\n        best_model_weights = None\n        \n        print(f\"\\n🚀 Starting Training (Max Epochs: {epochs}, Patience: {patience})\")\n\n        for epoch in range(epochs):\n            self.model.train()\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            # --- Progress Bar ---\n            with tqdm(train_loader, total=n_batches, unit=\"batch\", leave=True) as pbar:\n                pbar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n                \n                for batch_idx, (images, labels) in enumerate(pbar):\n                    images, labels = self._tf_to_torch_fast(images, labels)\n                    \n                    self.optimizer.zero_grad()\n                    \n                    # Mixed Precision Forward\n                    with torch.amp.autocast('cuda'):\n                        outputs = self.model(images)\n                        loss = self.criterion(outputs.logits, labels)\n                    \n                    # Mixed Precision Backward\n                    self.scaler.scale(loss).backward()\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n\n                    # Metrics\n                    batch_size = labels.size(0)\n                    running_loss += loss.item() * batch_size\n                    \n                    preds = torch.argmax(outputs.logits, dim=1)\n                    batch_correct = (preds == labels).sum().item()\n                    correct += batch_correct\n                    total += batch_size\n                    \n                    # Live Update of Bar\n                    current_acc = correct / total\n                    current_loss = running_loss / total\n                    pbar.set_postfix({\"loss\": f\"{current_loss:.4f}\", \"acc\": f\"{current_acc:.4f}\"})\n\n            # --- Validation & History ---\n            val_loss, val_acc = self._validate(val_loader)\n            \n            self.history['loss'].append(current_loss)\n            self.history['accuracy'].append(current_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_accuracy'].append(val_acc)\n            \n            print(f\"    Validation - loss: {val_loss:.4f} - acc: {val_acc:.4f}\")\n\n            # --- Early Stopping ---\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                best_model_weights = copy.deepcopy(self.model.state_dict())\n                # print(\"    ⭐ Saved Best Model\")\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"\\n🛑 Early stopping triggered! Restoring best weights.\")\n                    self.model.load_state_dict(best_model_weights)\n                    break\n        \n        # Restore best weights if finished naturally\n        if best_model_weights is not None and patience_counter < patience:\n             self.model.load_state_dict(best_model_weights)\n\n    def plot_history(self):\n        \"\"\"Plots accuracy and loss graphs similar to Keras.\"\"\"\n        if not self.history['loss']:\n            print(\"No training history to plot.\")\n            return\n\n        epochs_range = range(1, len(self.history['loss']) + 1)\n        \n        plt.figure(figsize=(12, 5))\n\n        # Plot Loss\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs_range, self.history['loss'], 'bo-', label='Training Loss')\n        plt.plot(epochs_range, self.history['val_loss'], 'r-', label='Validation Loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n\n        # Plot Accuracy\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs_range, self.history['accuracy'], 'bo-', label='Training Acc')\n        plt.plot(epochs_range, self.history['val_accuracy'], 'r-', label='Validation Acc')\n        plt.title('Training and Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n    def predict(self, tf_dataset):\n        \"\"\"Runs inference on a dataset and returns (y_true, y_pred_prob)\"\"\"\n        loader = tfds.as_numpy(tf_dataset)\n        self.model.eval()\n        \n        all_preds = []\n        all_labels = []\n        \n        print(f\"🔍 Running Prediction...\")\n        with torch.no_grad():\n            for images, labels in tqdm(loader, unit=\"batch\"):\n                images, labels = self._tf_to_torch_fast(images, labels)\n                \n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(images)\n                    \n                # Get probabilities (softmax)\n                probs = torch.softmax(outputs.logits, dim=1)\n                \n                all_preds.append(probs.cpu().numpy())\n                all_labels.append(labels.cpu().numpy())\n                \n        return np.concatenate(all_labels), np.concatenate(all_preds)\n\n    def save(self, path=\"vit_model.pth\"):\n        torch.save(self.model.state_dict(), path)\n        print(f\"💾 Model saved to: {path}\")\n\n    def load(self, path=\"vit_model.pth\"):\n        self.model.load_state_dict(torch.load(path))\n        self.model.eval()\n        print(f\"📂 Model loaded from: {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:50:14.070679Z","iopub.execute_input":"2025-12-12T11:50:14.071175Z","iopub.status.idle":"2025-12-12T11:50:32.790280Z","shell.execute_reply.started":"2025-12-12T11:50:14.071152Z","shell.execute_reply":"2025-12-12T11:50:32.789623Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 2.5 Loss and Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"## 2.6 Fitting\n","metadata":{}},{"cell_type":"markdown","source":"### 2.6.1 Early Stopping for keras models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',        # Watch validation loss (metric of truth)\n    patience=3,                # Wait 3 epochs before stopping (gives it a chance to recover)\n    restore_best_weights=True, # CRITICAL: Go back to the best weights, not the last ones\n    verbose=1                  # Print a message when it triggers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:51:02.935687Z","iopub.execute_input":"2025-12-12T11:51:02.936013Z","iopub.status.idle":"2025-12-12T11:51:02.940106Z","shell.execute_reply.started":"2025-12-12T11:51:02.935976Z","shell.execute_reply":"2025-12-12T11:51:02.939381Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Function for training, predicting and cleaning (llm) for keras models","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt  # Required to render plots inside a function\n\n# Initialize the global results dictionary if it doesn't exist\nif 'all_results' not in globals():\n    all_results = {}\n\n# Define your test sets map\ntest_sets = {\n    \"CIFAKE_Test\": ds_test_cifake,\n    \"OOD_Test\":    ds_test_ood\n}\n\ndef train_eval_clean(model, model_name, train_ds, val_ds, test_sets, epochs=30):\n    \"\"\"\n    1. Trains the model.\n    2. PLOTS the history (New Step).\n    3. Evaluates on all test/OOD sets.\n    4. Saves results to global 'all_results'.\n    5. Saves model to disk.\n    6. Deletes model and clears VRAM.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"🚀 STARTING PIPELINE FOR: {model_name}\")\n    print(f\"{'='*60}\")\n\n    # ---------------------------------------------------------\n    # 1. TRAIN\n    # ---------------------------------------------------------\n    print(f\"📉 Training {model_name}...\")\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        verbose=1,\n        callbacks=[early_stopping] # Assumes 'early_stopping' is defined globally\n    )\n\n    # ---------------------------------------------------------\n    # 1.5. PLOT HISTORY (Added)\n    # ---------------------------------------------------------\n    print(f\"📊 Plotting training history for {model_name}...\")\n    \n    # Convert history to pandas DataFrame\n    history_frame = pd.DataFrame(history.history)\n    \n    # Plot Loss\n    # We use plt.show() to force the plot to render immediately during function execution\n    history_frame.loc[:, ['loss', 'val_loss']].plot(title=f\"{model_name} - Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show() \n    \n    # Plot Accuracy (Checks if 'binary_accuracy' exists, falls back to 'accuracy' if needed)\n    acc_key = 'binary_accuracy' if 'binary_accuracy' in history_frame.columns else 'accuracy'\n    val_acc_key = f'val_{acc_key}'\n    \n    if acc_key in history_frame.columns:\n        history_frame.loc[:, [acc_key, val_acc_key]].plot(title=f\"{model_name} - Accuracy\")\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.show()\n\n    # ---------------------------------------------------------\n    # 2. EVALUATE\n    # ---------------------------------------------------------\n    print(f\"\\n🔍 Evaluating {model_name} on {len(test_sets)} datasets...\")\n    \n    # Create entry in global results dict\n    all_results[model_name] = {}\n\n    for ds_name, ds in test_sets.items():\n        print(f\"   • Predicting on {ds_name}...\")\n        \n        # A. Predict\n        preds = model.predict(ds, verbose=0) # verbose=0 to keep logs clean\n        \n        # Handle shapes: (N, 2) -> (N,) or (N, 1) -> (N,)\n        if preds.shape[-1] > 1:\n            y_pred = preds[:, 1] \n        else:\n            y_pred = preds.flatten()\n            \n        # B. Get True Labels\n        y_true = np.concatenate([y for x, y in ds], axis=0)\n        \n        # C. Store\n        all_results[model_name][ds_name] = {\n            'y_pred': y_pred,\n            'y_true': y_true\n        }\n\n    # ---------------------------------------------------------\n    # 3. SAVE TO DISK\n    # ---------------------------------------------------------\n    filename = f\"{model_name}_final.keras\"\n    model.save(filename)\n    print(f\"\\n💾 Model saved to: {filename}\")\n\n    # ---------------------------------------------------------\n    # 4. CLEANUP (The most important part)\n    # ---------------------------------------------------------\n    print(f\"🧹 Scrubbing VRAM...\")\n    del model\n    tf.keras.backend.clear_session()\n    gc.collect()\n    print(f\"✨ {model_name} pipeline complete. GPU is ready for next model.\\n\")\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:51:05.675773Z","iopub.execute_input":"2025-12-12T11:51:05.676537Z","iopub.status.idle":"2025-12-12T11:51:05.686001Z","shell.execute_reply.started":"2025-12-12T11:51:05.676510Z","shell.execute_reply":"2025-12-12T11:51:05.685344Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### 2.6.2 Custom CNN","metadata":{}},{"cell_type":"code","source":"hist_custom = train_eval_clean(\n    custom_cnn, \n    \"Custom_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=30 # Change to 30 for real run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:51:20.854332Z","iopub.execute_input":"2025-12-12T11:51:20.854992Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n🚀 STARTING PIPELINE FOR: Custom_CNN\n============================================================\n📉 Training Custom_CNN...\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1765540287.105335     100 service.cc:148] XLA service 0x7ad04c413dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1765540287.110854     100 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1765540287.110874     100 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1765540287.868625     100 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"      1/Unknown \u001b[1m19s\u001b[0m 19s/step - accuracy: 0.5469 - loss: 0.7118","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765540299.472718     100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"   3750/Unknown \u001b[1m574s\u001b[0m 148ms/step - accuracy: 0.6703 - loss: 0.5836","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m724s\u001b[0m 188ms/step - accuracy: 0.6703 - loss: 0.5836 - val_accuracy: 0.6483 - val_loss: 0.6354\nEpoch 2/30\n\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 130ms/step - accuracy: 0.7560 - loss: 0.4740 - val_accuracy: 0.7082 - val_loss: 0.5726\nEpoch 3/30\n\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 128ms/step - accuracy: 0.7837 - loss: 0.4315 - val_accuracy: 0.7090 - val_loss: 0.5621\nEpoch 4/30\n\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 131ms/step - accuracy: 0.8034 - loss: 0.3997 - val_accuracy: 0.6987 - val_loss: 0.6368\nEpoch 5/30\n\u001b[1m1506/3750\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4:18\u001b[0m 115ms/step - accuracy: 0.8162 - loss: 0.3804","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### 2.6.3 Pre-trained CNN","metadata":{}},{"cell_type":"code","source":"hist_cnn = train_eval_clean(\n    model_cnn, \n    \"Pretrained_CNN\", \n    ds_train_full, \n    ds_val_full, \n    test_sets,\n    epochs=30 # Change to 30 for real run\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:10:54.227783Z","iopub.execute_input":"2025-12-12T10:10:54.228115Z","iopub.status.idle":"2025-12-12T10:29:01.757573Z","shell.execute_reply.started":"2025-12-12T10:10:54.228087Z","shell.execute_reply":"2025-12-12T10:29:01.756903Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n🚀 STARTING PIPELINE FOR: Pretrained_CNN\n============================================================\n📉 Training Pretrained_CNN...\nEpoch 1/2\n\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 96ms/step - accuracy: 0.7314 - loss: 0.5168 - val_accuracy: 0.7561 - val_loss: 0.4935\nEpoch 2/2\n\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 93ms/step - accuracy: 0.7620 - loss: 0.4738 - val_accuracy: 0.7595 - val_loss: 0.4850\nRestoring model weights from the end of the best epoch: 2.\n\n🔍 Evaluating Pretrained_CNN on 2 datasets...\n   • Predicting on CIFAKE_Test...\n   • Predicting on OOD_Test...\n","output_type":"stream"},{"name":"stderr","text":"Invalid SOS parameters for sequential JPEG\nInvalid SOS parameters for sequential JPEG\n","output_type":"stream"},{"name":"stdout","text":"\n💾 Model saved to: Pretrained_CNN_final.keras\n🧹 Scrubbing VRAM...\n✨ Pretrained_CNN pipeline complete. GPU is ready for next model.\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### 2.6.3 Visual Transformer","metadata":{}},{"cell_type":"code","source":"# 1. Initialize\nvit_model = ViTClassifier(num_labels=2)\n\n# 2. Train (Pass val set and patience)\n# Note: Ensure datasets are Unbatched in TF or handle batching inside appropriately.\n# Since my class assumes tfds.as_numpy iterates batches, ensure your input DS is BATCHED.\nvit_model.train(ds_train_full, ds_val_full, epochs=20, patience=3)\n\n# 3. Plot\nvit_model.plot_history()\n\n# Predictions on test sets\n# 1. Initialize the entry for this specific model\nmodel_name = \"ViT_Base_Frozen\"\nall_results[model_name] = {}\n\nprint(f\"🚀 Starting Evaluation for {model_name}...\")\n\n# 2. Iterate through your existing test_sets map\nfor ds_name, ds in test_sets.items():\n    print(f\"   • Predicting on {ds_name}...\")\n    \n    # Run inference using the class method\n    # This returns (True Labels, Predicted Probabilities [N, 2])\n    y_true, y_probs = vit_model.predict(ds)\n    \n    # We want the probability of the \"Positive\" class (Index 1)\n    # y_probs is shape [N, 2], so we take all rows, column 1\n    y_pred = y_probs[:, 1]\n    \n    # 3. Store in the global dictionary\n    all_results[model_name][ds_name] = {\n        'y_pred': y_pred,\n        'y_true': y_true\n    }\n\nprint(f\"\\n✅ Results for {model_name} saved to 'all_results'.\")\n\n# Saving the model\n# Save with a specific filename\nvit_model.save(\"ViT_Frozen_Base_Final.pth\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation for ViT_Base_Frozen...\n   • Predicting on CIFAKE_Test...\n🔍 Running Prediction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/313 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9df116db03b4dc087e38a4f191dd8a3"}},"metadata":{}},{"name":"stdout","text":"   • Predicting on OOD_Test...\n🔍 Running Prediction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/451 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39955d095b7d4b66bb772ea2cde89dba"}},"metadata":{}},{"name":"stderr","text":"Invalid SOS parameters for sequential JPEG\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Results for ViT_Base_Frozen saved to 'all_results'.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# 3. Results","metadata":{}},{"cell_type":"markdown","source":"### Note for Johannes/Anna.\n\nthe results for all the models can be accessed through the dictionary `all_results`. If you want to load in trained model then use the `tf.keras.models.load_model(\"model_name\")` for the custom CNN and the pre-trained cnn. This way you can make more predictions with the models. ","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Hold-out set performance ","metadata":{}},{"cell_type":"code","source":"# the hold-out set is the ds_test_cifake\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:31:18.481018Z","iopub.execute_input":"2025-12-12T11:31:18.481884Z","iopub.status.idle":"2025-12-12T11:31:18.503061Z","shell.execute_reply.started":"2025-12-12T11:31:18.481857Z","shell.execute_reply":"2025-12-12T11:31:18.501816Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2084674636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the hold-out set is the ds_test_cifake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cifake'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyError\u001b[0m: 'cifake'"],"ename":"KeyError","evalue":"'cifake'","output_type":"error"}],"execution_count":31},{"cell_type":"markdown","source":"## 3.2 Out-of-distribution set performance","metadata":{}},{"cell_type":"code","source":"# the ood set is the ds_test_ood","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Performance on custom images ","metadata":{}},{"cell_type":"code","source":"# use a pic of johannes and a pic of ai generated johannes to see if it predicts correctly ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Conclusion/Discussion","metadata":{}},{"cell_type":"markdown","source":"possible discussion points:\n> fine tune the base layers in both pre-trained classifier and visual transformer rather than just the classifier head (time/memory constraints).\n\n\n> Modify the custom cnn to see how simple it can be made while still maintaining good performance. ","metadata":{}},{"cell_type":"markdown","source":"# 5. References","metadata":{}},{"cell_type":"markdown","source":"# 6. Division of Labour","metadata":{}}]}